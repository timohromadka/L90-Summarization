{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "tflhJHciImh-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TODO\n",
        "\n",
        "1. Implement beam search\n",
        "2. Write down hyperparameters and come up with experiments to run\n",
        "3. Generate graphs for paper\n",
        "4. Write paper sections\n",
        "5. modularize code"
      ],
      "metadata": {
        "id": "2LEh1JWVSMQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwvAkWRsiFYq",
        "outputId": "dfdba403-0eae-42a9-f986-81084fc2bdc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=ff54fd16410bcd6888b5b5d717fe1eda143fe17f41dfdb4e110a3e725e80510b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba\n",
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwORIDw_oAxV",
        "outputId": "25d8c87b-3827-42fa-cb25-a32e6b56a487"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.41.1)\n",
            "Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Cambridge/L90')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNmU_MB2iGPt",
        "outputId": "42125a42-5c0e-4396-903c-a1da7da87850"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVI8VFeWiM-4",
        "outputId": "b3d7c223-07d0-4a4e-fabf-a91b59186b09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 29 14:38:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import math\n",
        "\n",
        "import spacy\n",
        "from torch.nn import Transformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k, WikiText2\n",
        "from typing import Iterable, List\n",
        "from torch.utils.data import DataLoader, TensorDataset, dataset\n",
        "from torch.cuda.amp import GradScaler, autocast  # For mixed precision training\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "bZzspE7DiSUr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/train.json\", 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(\"data/validation.json\", 'r') as f:\n",
        "    validation_data = json.load(f)\n",
        "\n",
        "with open(\"data/test.json\", 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# ================= LOAD DATASET ===========================\n",
        "train_articles = [article['article'] for article in train_data]\n",
        "train_summaries = [article['summary'] for article in train_data]\n",
        "\n",
        "val_articles = [article['article'] for article in validation_data]\n",
        "val_summaries = [article['summary'] for article in validation_data]\n",
        "\n",
        "\n",
        "test_articles = [article['article'] for article in test_data]\n",
        "test_summaries = [article['summary'] for article in test_data]\n",
        "\n",
        "# ================= REDUCE SIZE ===========================\n",
        "size_of_dataset = 5000\n",
        "train_articles = train_articles[:size_of_dataset]\n",
        "train_summaries = train_summaries[:size_of_dataset]\n",
        "\n",
        "val_articles = val_articles[:int(size_of_dataset/5)]\n",
        "val_summaries = val_summaries[:int(size_of_dataset/5)]\n",
        "\n",
        "\n",
        "test_articles = test_articles[:int(size_of_dataset/5)]\n",
        "test_summaries = test_summaries[:int(size_of_dataset/5)]"
      ],
      "metadata": {
        "id": "pqZt9LDvi5pZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED_NUM = 42\n",
        "\n",
        "torch.manual_seed(SEED_NUM)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED_NUM)\n",
        "\n",
        "# Model parameters may need to be adjusted based on the specifics of the summarization task\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 4\n",
        "FFN_HID_DIM = 256\n",
        "BATCH_SIZE = 8\n",
        "NUM_ENCODER_LAYERS = 1\n",
        "NUM_DECODER_LAYERS = 1"
      ],
      "metadata": {
        "id": "CcwADV_W25lu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "V6tlF4VGi_G5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "mCCSD1g3jIQZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Data Preparation"
      ],
      "metadata": {
        "id": "tflhJHciImh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "token_transform = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
        "    for data_sample in tqdm(data_iter, desc='Tokenizing data.'):\n",
        "        yield token_transform(data_sample)[:(EMB_SIZE-2)]\n",
        "\n",
        "# Create torchtext's Vocab object for English\n",
        "vocab_transform = build_vocab_from_iterator(yield_tokens(train_articles),\n",
        "                                            min_freq=1,\n",
        "                                            specials=special_symbols,\n",
        "                                            special_first=True)\n",
        "\n",
        "vocab_transform.set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "EBuIbboli-Nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "212bfb11-580e-4918-beb5-00016a5e0c42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing data.: 100%|██████████| 5000/5000 [00:18<00:00, 275.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting the model for English only\n",
        "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = len(vocab_transform)\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9NrZwpBjJu_",
        "outputId": "c4a92895-ec79-4d37-efa2-a278e3dc44e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Icy847L4s-",
        "outputId": "049c255d-75f8-492b-ce7e-324d2654bcbf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52516"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "# Function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    tensor = torch.cat((torch.tensor([BOS_IDX]),\n",
        "                        torch.tensor(token_ids)))\n",
        "    # Ensure the tensor is at most 254 in length to accommodate EOS\n",
        "    if len(tensor) >= 255:\n",
        "        tensor = tensor[:255]\n",
        "    tensor = torch.cat((tensor, torch.tensor([EOS_IDX])))\n",
        "    # Pad if less than 256\n",
        "    final_tensor = torch.cat((tensor, torch.tensor([PAD_IDX] * (256 - len(tensor))))) if len(tensor) < 256 else tensor\n",
        "    return final_tensor\n",
        "\n",
        "\n",
        "# src and tgt language text transforms\n",
        "text_transform = sequential_transforms(token_transform,  # Tokenization\n",
        "                                       vocab_transform,  # Numericalization\n",
        "                                       tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "def pad(tensor, length):\n",
        "    if len(tensor) < length:\n",
        "        return torch.cat([tensor, torch.full((length - len(tensor),), PAD_IDX)])\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_processed = text_transform(src_sample)\n",
        "        tgt_processed = text_transform(tgt_sample)\n",
        "\n",
        "        # Truncate if longer than 256, else pad\n",
        "        src_batch.append(src_processed[:EMB_SIZE] if len(src_processed) > EMB_SIZE else pad(src_processed, EMB_SIZE))\n",
        "        tgt_batch.append(tgt_processed[:EMB_SIZE] if len(tgt_processed) > EMB_SIZE else pad(tgt_processed, EMB_SIZE))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    src_batch = torch.stack(src_batch)\n",
        "    tgt_batch = torch.stack(tgt_batch)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "NIZ3kF8tjKpp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataLoader instances\n",
        "train_dataloader = DataLoader([(article, summary) for article, summary in zip(train_articles, train_summaries)],\n",
        "                        batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0, shuffle=True)\n",
        "\n",
        "# Creating DataLoader instances\n",
        "val_dataloader = DataLoader([(article, summary) for article, summary in zip(val_articles, val_summaries)],\n",
        "                        batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0, shuffle=True)"
      ],
      "metadata": {
        "id": "V_Rek-TxjS_T"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step, (src, tgt) in enumerate(tqdm(train_dataloader, desc='Training')):\n",
        "    for t in src:\n",
        "        print(t)\n",
        "        print(len(t))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbNHZtDCX4uB",
        "outputId": "bc327c1b-841c-406a-829c-00789e8aa8cb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/625 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    2,    68,     4,  2168, 16754,     6,  1717,  2853,  6737,     4,\n",
            "        17413,   249,    35,  1035,  1099,     7,  4935,    10,     5,  1205,\n",
            "          545,     5,   357,  3582,   135,    50,    39,   600,    11,     8,\n",
            "        12416,    62,  1723,   474,    73,     6,    32,    13,   430,   160,\n",
            "            4,  1502,  6318,     9,  6360,  1575, 20359,  2232,  2673,    31,\n",
            "         1323,    14, 22837,     6, 15801,    10,  2650,     7,    35,  1035,\n",
            "            7,  9313,    15,     7,  5868,     4,    76,  3114,  1169,  5174,\n",
            "            5,   780,     7,  7409,     5,  4863,    84,   230,    23, 15907,\n",
            "           72,  2192,    10,  2803, 50198,     4,    86,  2834,    27,    32,\n",
            "           13,    62, 44686,    73,     7,   531,  3342,  1099,     7,     5,\n",
            "         1205,   120,    50,    93,   938,  1937,     6,   119,   592,  4077,\n",
            "            6, 23570,   881,    10,  9313,  1024,  3476,  5868,     4,   127,\n",
            "        20359,    27,  2411,     6,  2995,   403,     9,     5,  1205,    93,\n",
            "           89,  9468,  1837,     6,   133,  1530,    10, 15917,    11, 15659,\n",
            "           12,   353,    32,   532,   628,  4967,    84,   674,     7,     8,\n",
            "          152,     9,   997,    36,   672,   851,     4,   127, 20359,     6,\n",
            "           38,    13, 12030,    11,     8,   649,     9,  1320,    23,  2293,\n",
            "           56,   170,    36,   984,   471,    46, 11034,     6,  1053,    59,\n",
            "           25,     8,   907,    15, 48736,  1941,    11,   177,     4,    76,\n",
            "           27,  2411,     6,  2995,   403,     9,     5,  1205,    93,    89,\n",
            "         9468,  1837,     6,   133,  1530,    10, 15917,    11, 15659,   168,\n",
            "          353,    32,   532,   628,  4967,    84,   674,     7,     8,   152,\n",
            "            9,   997,    36,   672,   851,     4,  3378,  1205,  1099,    93,\n",
            "         5496,  3342,    11,   523,    14,  1496,    10,  3087,    14,  9381,\n",
            "           26,   187,    26,    14,   353, 10100,  6530,  4120,     7,    44,\n",
            "          743,     6,    29,    27,     4,     3])\n",
            "256\n",
            "tensor([    2,  4890,     6,   253,    41,    69,    42,    51,   304,    39,\n",
            "          401,   749,    11,     5,   141, 11136,    23,    26,   240, 10165,\n",
            "           26,     5,  2233,    12,  5411,  1358,     4, 11568, 18644,    22,\n",
            "            5,  1492,    10,   682,  7752,    14,  2233,    12, 23751,    39,\n",
            "         9459,  9273,  7784,     4,    18,  5295,    39,  1026,    10,  1041,\n",
            "           11,  2952,  4976,     6,    50,   439,  1945,    31,     5,  8191,\n",
            "        22576,  2233,    12,  4276,     6,    10,    14,   136,     9,   411,\n",
            "            5,  5124,   101,  1098,   159,     7,   106,    19,    11,     5,\n",
            "         3493,     9,  9976, 10995,     4,   103,     8,  1132,     7,  3006,\n",
            "            8,  4398,  4099,    72,     5,  2233,    12,  5411,   141,    40,\n",
            "          500,  6346,    37,   705,    23,   920,  1273, 11366,     6,     5,\n",
            "          726,     9,   606,    17, 37502,  2774,  1266,  6735,    12,     5,\n",
            "         2364,  5411,  1492,   274,    11,     5,   141,     4,    18,   274,\n",
            "         3854,    11, 32972,    10,    11,    88,    87,    28,  2103,   149,\n",
            "          654,    17,   303, 23751,    10,    66,  1287,    10,  3210, 23751,\n",
            "           77,   221,   113,  5411,  2393,     4,    65,  1225,     5, 18565,\n",
            "          541,     7,  2226,     4,    86,     6,   117,    17,     8,  3054,\n",
            "            4,   103,    56,  1358,    16,  4656,  7714,    16,   459,   119,\n",
            "           16, 10180,  3585,    16,   521,    75,    25,   338,  6562,   541,\n",
            "            6,  7598,     4, 11366,   814,    85,  1184,   193,  1158,    64,\n",
            "            5,  4377,     5,   274,    19,   419,    15,     6,     5,    80,\n",
            "           29,    17,   419,    14,     6,    75,   154,   240,   354,  4248,\n",
            "           12,   235, 44723,   786,     7,  2554,     4,    21,   153,  2543,\n",
            "           72,   195,  4270, 22021, 13243,    23,   183,  5120,     4,   153,\n",
            "          448,    32,  9263,    25,   367,   135,   101,    84,   129,     7,\n",
            "         2084,    80,    64,   714,     8,     3])\n",
            "256\n",
            "tensor([    2,  1592,    10,     8,   333,    79,    36,    44,  1938,    11,\n",
            "            5,  1148,     6,   312,   259,    39,   185,     7,  1431,    11,\n",
            "          679,    23,     5,  1003,   258,     4,    18,   871,  1936,    34,\n",
            "          306,     8,   217,   175,    11,     8,   723,    87,     4,  7539,\n",
            "         2865,  3446,    28,   390,   527,    63,   715,  7821,     4, 11643,\n",
            "            7,    82,     6,  6221, 14702,   139, 22718,     4,   745,     6,\n",
            "         1370,     6,   754, 51890,     4,  1070,     8,  1622,     6,    32,\n",
            "          754,  3698,  3590,    14,     5,   298,     4,  9757,  8479,    10,\n",
            "         7185,  6150,  1998,   312,   259,   579,   557,     9,  1557,  1363,\n",
            "           15,   199,   162,     4,   312,   259,  1122,  9757,  8479,    41,\n",
            "           99,    42,  3681,     8,  4361,    23,   433,  2865,  3446,    15,\n",
            "          208,     4,    18,   312,   259,  1016, 22649,   121,    44,  3360,\n",
            "           12,    60,    11,   579,    15,   208,    11,   312,     4,  7539,\n",
            "         2865,  3446,    41,   142,    42,    52,   886,    96,   834,  6150,\n",
            "           41,    99,    42,    19,  3271,   147,   176,  2564,    15,   199,\n",
            "            4,   103,     5, 44893,    25,     5,  7492,  1650,     6,    50,\n",
            "          119,    48,  5366,    64,   259,    70,  7323,   593,   331,     5,\n",
            "         1583,   271,     4, 15104,     5,  1003,   258,   754,   831,    14,\n",
            "            5,   298,    70,  1868,    53,   117,    19,   126,   485, 22411,\n",
            "          123,    64,     8,   593,    20,   364,    92,    57, 15519,  2177,\n",
            "            5,   223,  3212,     4,   574,    19,   831,     6,  1370,     6,\n",
            "           19,     7,  1848,  2593,     4,  6455,     5,   175,   259,   630,\n",
            "          121,   240,     9,    56,   232,    70,   223,  2304,     6,    50,\n",
            "           52,  2337,    35, 10909,     7,    35,    11,     5,    74,   362,\n",
            "           25,    94,     4,    65,    19,   703,     6,  5857,     6,    20,\n",
            "          533,     9,     8,  5744,  1282,     3])\n",
            "256\n",
            "tensor([    2,    68,     4,  1115,  4733,     4,   115,    22,     4, 16274,\n",
            "           61,     6,   339,   272,   300,     4,   114,     4,   112,    22,\n",
            "            4, 19396,    61,     6,   339,   272,   300,     4,   397,    91,\n",
            "           34,   422,  7831,   154,   877,    80,   165,     5,  4601,   449,\n",
            "           35,  3406,     7,    57,   242,     6,     8,    96, 10016,  4366,\n",
            "           93,  5308,   257, 18120,     4, 26360,   307,     6,     8,  1371,\n",
            "            9,   554,  4709,  2547,  3257,   123,    60,     9,  3370,     6,\n",
            "         3326,   154,  1760,  7485,    19,  3406,     7, 17276, 24854,    10,\n",
            "          154,  5917,  4307,    19,  1233,    23,  2524, 12578,     6,    26,\n",
            "          187,    26,     8,  7245,   113,  4616,     4,    65,   684,    25,\n",
            "         3498,     9,   779,    10,  1234,    11,     8,  1132,     7,  5340,\n",
            "          307,    70,   607,    26,     8,  1107,   133,  2776,     4, 26360,\n",
            "          307,     6,     8,  1371,     9,   554,  4709,  2547,  3326,   154,\n",
            "         1760,  7485,    19,  3406,     7, 17276, 24854,    10,   154,  5917,\n",
            "         4307,    19,  1233,    23,  2524,  3289,     6,    26,   187,    26,\n",
            "            8,  7245,   113,  4616,     4,    68,  7260,    63, 18664,     6,\n",
            "         5192,    63,     4,   219, 30388,     6,    15,   227,     9,    57,\n",
            "          242,     6,    80,    95,   916,   248,   154,     4,     5,    63,\n",
            "         2547, 15408,    11,  1807,     9,  6309,    87,    10, 48299,     4,\n",
            "         5879,  1547, 12364,    39,    89,  1588,     4,   318,     5, 48982,\n",
            "           70,  8753,  8060,  3694,     4,  1865,     6,   854,    95,  1218,\n",
            "            8, 46787,  3576,   197,     5,    63,    80,     6,     4,    54,\n",
            "          364,  9322,    10,   133,  1824,     4,   161,     4,   626,    95,\n",
            "           35,  6687,    26,     8,  2048,     9,   133,  2345,     6,     7,\n",
            "         5340,     4,   667,  3966,     4,    55,    62,  2101,    73,  1265,\n",
            "        44216,   364,     5, 18664,    31,     3])\n",
            "256\n",
            "tensor([    2,    55,  3596, 13281,    28,   939,   931,    24,   762, 15206,\n",
            "          100,    11,    37,  2952,  1343,  3215,    14,   148,   293,   222,\n",
            "            4,    18,  4176, 14850, 11398,    12,  2676,  4385,    19,  2036,\n",
            "           15, 43946,  2868,    11,  1343,    17,  1997,    10,    13,   931,\n",
            "          109,    31,   151, 35684,   726,   158,   345, 41076,    12, 12901,\n",
            "            4, 27196,   676,  1655,   906,    17,  5058, 27196,    12,    38,\n",
            "           19,   340,     5,  1084,    12,  1210,     7,  1252,     7,   286,\n",
            "          281,   584,    64,     5,   812,     4,    18,  4176, 14850,   100,\n",
            "           19,     5,  3421,   284,    11,     5,  3215,     7,    35,   931,\n",
            "           14,   148,   293,   222,    75,    66,    11,     5,   441,   182,\n",
            "          233,     4,    18, 11398,    12,  2676,   100,  4327,   246,  3935,\n",
            "           23, 29555,  9843,     6,    10,    13,  1026,   145, 24087,     4,\n",
            "           86,     8,  1191,    81,   158, 25421,   158,    20,    32,    13,\n",
            "          931,    14,   148,   293,   222,    36,    32,    13,   564,   242,\n",
            "         1139,     9,   148,   602,   222,    13,  2964,    11,   583,   336,\n",
            "            4,   161,    19,     5,  3421,   812,    11,     5,  4176, 14850,\n",
            "          377,     7,    34,   931,    25,   148,   293,   222,    75,    66,\n",
            "           56,    47,     6,    54,   852,    12,    36,   207,    17,  1084,\n",
            "           12,    64,   148,  1240,   222,   979,     9,   676,  1655,    28,\n",
            "           43, 30661,    11,     5, 11860,  3215,    11,     5,   441,   182,\n",
            "          233,     4,    18,   284,     6,    54,    13,  1026,   145, 24087,\n",
            "            6,  4327,   246,  3935,    10,    19,    57,     9,   149,  1040,\n",
            "           11,     5,  3215,     7,  1139,     8,   703,  1265,     9,     5,\n",
            "         1343,  6211,   479,    10, 14537,  2076,     4,    65,    13,  5601,\n",
            "           31,  5599, 11829, 37037, 25412,     7,   673,    88, 11398,  1384,\n",
            "            6,   200,     8,  7417, 12388,     3])\n",
            "256\n",
            "tensor([    2, 39481, 39115,     6,   454,    41,    69,    42,    51,    55,\n",
            "          601,  1455,     9,   663,   754,    28,    43,   107,    11,     5,\n",
            "         2923,     9,     5,   641,    20, 21683,  1228, 20238,    13,  1402,\n",
            "           58,    29,  2837,    74,    47,     6,     8,   436,  3926,  3621,\n",
            "         5660,   372,    27,   251,     4,    18,  2923,     9, 21683,  1228,\n",
            "        20238,    17,   641,    13,   107,    11,   454,    17,  3725,  4171,\n",
            "           15,   251,     4,    55,   929,   179,    20,    13,  8110,     5,\n",
            "         2923,     6,    54,    13,   107,   243,    25,    37,  7199,     9,\n",
            "           64,  2237,   771,    11,     5,  3725,  4171,   294, 20707, 12899,\n",
            "            6,   107,    21,   195,   418,    21,   754,   444,     5,  4941,\n",
            "            6,  2479, 14774,  4565,   775, 27458,    27,     4,  5881,   412,\n",
            "            5,   754,    46,   647,    14,    37,  5343,     6, 27458,    27,\n",
            "            6,    21,    40,   519,     5,  4335,    52,    35,   468,     7,\n",
            "          119,   132,   206,     4,    21,  2017,   251,     6, 39559,   498,\n",
            "         3047,   345,  4281,    27,     5,   863,    12,  2031, 35006,   634,\n",
            "            7,    34,  1491,    72,     5,   265,     9,     8,  2798,    11,\n",
            "            5,  3725,  4171,    11,  1640,   454,     6,    10,     5,  1366,\n",
            "           13,    21,   111,  1317,    40,  3320,   977,    84,   500,  1435,\n",
            "          267,    30,    32,     4,    21,   218,  4179,     8,  4366,     9,\n",
            "            5,   757,   477,  2374,     4, 20238,    13,    74,   260,     5,\n",
            "          356,     9,   289,   388,     6,  1068,     6,    58,    29,   191,\n",
            "          109,    30,     5, 10411,    12,  9685, 14882,   331, 39992,     6,\n",
            "         4171,     6,    15,   125,    29,    27,    84,    35,     8, 12259,\n",
            "          552,    78,     5,  3725,  4171,     4,   218,  1858,   154, 12346,\n",
            "         2036,     5,  2923,  2374,     4,  3384, 22526,    11,    15,     5,\n",
            "          377,   294, 20707, 12899,    15,     3])\n",
            "256\n",
            "tensor([    2,    16,  1070,  4097,    10,   118,    16,    22,  1639,  3107,\n",
            "           28,   580,    33,   152,   145,   521,    53,  1624, 18864,    36,\n",
            "          235,   275,     4,    18,   181,    25,     5,   635,     9,     5,\n",
            "          830,  1321,   178,    12,   474,   491,  1629,    13,   475,     7,\n",
            "         7275,    33,   100,    10,    62,  1624,    14,    33,   152,    73,\n",
            "            6,    33,  6245,   430,    74,   162,     4,  1639,  3107,     6,\n",
            "          730,     6,  2232,  3858,   145,     5,  4354,    58,    49,  1245,\n",
            "        12349,   188,  3013,   984,    49,  4390,    23,     5,   830,   104,\n",
            "          367,     6,   200,    26,     8,  7704,     9,   482,   178,   719,\n",
            "          405,     6,   116,   419,    26,     8,  1981,    62,   491,  7013,\n",
            "           73,    14,   875,  6961,  5919,  8403,     4,   698,   162,  1639,\n",
            "           70,  6245,  9571,  3107, 38935,     6,     8, 11999, 17253,    38,\n",
            "          944,  1465,    33,     6,  1053,    14,     5,    83,    87,     7,\n",
            "         1184,   154,  4403,  1639,  2678,    33,   719,   100,   434,   233,\n",
            "          285,  1594,  1624,     9, 18864,    30,  4325,  8403,    10,    24,\n",
            "         2595,    10,  1449,   469,     4,    62,   150,   972,   354,   188,\n",
            "         3013,    46,   747,    59,   111,    49,  3261,    60,    10,  2678,\n",
            "            6,    73,  9571,    27,     4,    62,   150,    19, 15535,    14,\n",
            "           33,   152,     4,   150,    28,  9833,     8,   648,     9,  1449,\n",
            "           10,   195,  2595,    80,     4,  2939,  1846,   125,    50,    93,\n",
            "          119,   143,    73,  9571,     6,     5,  1060,     9,  1639,    70,\n",
            "          288,  2871,     6,    27,    33,  8189,    19,   108,   600,    11,\n",
            "            8,  7758,    62,    11,     5,  1298,     9,  4664,    73,    11,\n",
            "         1340,    23,    33,   385,    10,   104,   263,   173,     4,    62,\n",
            "          150,   328,   386,   644,  1606,     6,    73,  2009,  1175,  9571,\n",
            "          464,     4,    62,   304,    39,     3])\n",
            "256\n",
            "tensor([    2,    41,    69,    42,    51,    18,  3196,   107,  2955,    11,\n",
            "            8,  3492,    15,     8,   641,    11,     5,   155,  3900,  3438,\n",
            "           15,   207,   205,    34,  3278,    15,  1404,  2224,     7,   159,\n",
            "          117,     6,  1652,   324,    27,   224,     4,    18,  3196,     6,\n",
            "          481,    23,     8,   996,  1280,   107,    11,     5,   155,  4440,\n",
            "            6,  1100,     7,    34,    43,  1285,     7, 11997,    15,    44,\n",
            "          252,     6,   396,   977,   302,     7,   225,   106,   109,     6,\n",
            "            5,   227,   738,   479, 11965,   372,    81,    69,     4,    21,\n",
            "           65,    19,   137,  5406,    20,   354,  1971,   139,    48,   427,\n",
            "         7015,     7, 11997,   106,     6,    21,    27,   345,  8921,     6,\n",
            "          273,  1113,   282,    17,  2209,    14,  4646,   357,    10, 11965,\n",
            "            4,   144,  1385,   519,   655,  1470,  1456,  4236,  5916,  5250,\n",
            "          655,    12, 11310,     6,   602,     6,    19,  1233,     7,    20,\n",
            "         3492,    10,   242,    57,   107,    15,     8,   216,  7197,    11,\n",
            "          307,    17,   829,  2763,  1437,    15,   207,     6,     8,   633,\n",
            "          372,     6,    38,    13, 10741,    31,   417,     6,    81,    69,\n",
            "          224,     4,  1758, 10074,    46,  4599,     7, 51422,    11,  1636,\n",
            "            6,  2078,     4,   963,    12, 11310,     6,    38,    19,   315,\n",
            "            7,    35,    11,  5961,     6,    19,     8,  2383,    38,    13,\n",
            "          228,    15,  2383,  4153,    17,   736,     9,   136,   524,   976,\n",
            "           11,   349,   882,     4,    76,    19,    89,   564,     7,    35,\n",
            "            5,  4196,    38,  1285,    74,    47,    17,   689,   665,   912,\n",
            "         5040,  1456,     4, 42159,     6,    37,  6010,   608,   391,    11,\n",
            "         5961,    13,   365,   224,     6,   481,    23,    33,   172,     6,\n",
            "          146,     7,    33,   288,     6,  3718,   963,    12, 41992,     4,\n",
            "          150,    13,   423,  1059,    26,     3])\n",
            "256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Old Data Preparation"
      ],
      "metadata": {
        "id": "zv48jLjlIqoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "token_transform = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
        "    for data_sample in tqdm(data_iter, desc='Tokenizing data.'):\n",
        "        yield token_transform(data_sample)\n",
        "\n",
        "# Create torchtext's Vocab object for English\n",
        "vocab_transform = build_vocab_from_iterator(yield_tokens(train_articles),\n",
        "                                            min_freq=1,\n",
        "                                            specials=special_symbols,\n",
        "                                            special_first=True)\n",
        "\n",
        "vocab_transform.set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3qmYB_vI1C8",
        "outputId": "aea9bdbd-68ca-4123-9e98-d1445a5de061"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing data.: 100%|██████████| 5000/5000 [00:17<00:00, 284.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT3leOg_LWff",
        "outputId": "d30c4345-067c-4e2b-d0fe-7961fd0be9af"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86565"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting the model for English only\n",
        "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = len(vocab_transform)\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuk-LFtUI1E8",
        "outputId": "9cb450c6-49b8-44e2-a500-4f3bb05d57e5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = sequential_transforms(token_transform, #Tokenization\n",
        "                                    vocab_transform, #Numericalization\n",
        "                                    tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "def pad(tensor, length):\n",
        "    if len(tensor) < length:\n",
        "        return torch.cat([tensor, torch.full((length - len(tensor),), PAD_IDX)])\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform(src_sample))\n",
        "        tgt_batch.append(text_transform(tgt_sample))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "SpCpDtrhI1HF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataLoader instances\n",
        "train_dataloader = DataLoader([(article, summary) for article, summary in zip(train_articles, train_summaries)],\n",
        "                        batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0, shuffle=True)\n",
        "\n",
        "# Creating DataLoader instances\n",
        "val_dataloader = DataLoader([(article, summary) for article, summary in zip(val_articles, val_summaries)],\n",
        "                        batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0, shuffle=True)"
      ],
      "metadata": {
        "id": "5LdYIOzEI1Wy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step, (src, tgt) in enumerate(tqdm(train_dataloader, desc='Training')):\n",
        "\n",
        "    print(len(src))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-wHurZQiQr-",
        "outputId": "06c7cf02-c476-45e3-d45e-9b0db6797ad2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/625 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "xd3RHQteItVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HvS9OBPluPb8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_dataloader, optimizer, scaler, grad_accumulate_steps=2):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, (src, tgt) in enumerate(tqdm(train_dataloader, desc='Training')):\n",
        "        with autocast():  # Mixed precision training\n",
        "            src = src.to(DEVICE)\n",
        "            tgt = tgt.to(DEVICE)\n",
        "\n",
        "            # assert src.shape[1] == EMB_SIZE, \"To avoid unnecessary overhead, please truncate src tensors to the expected embedding size.\"\n",
        "            # assert tgt.shape[1] == EMB_SIZE, \"To avoid unnecessary overhead, please truncate tgt tensors to the expected embedding size.\"\n",
        "            # assert src.shape[0] == BATCH_SIZE, f\"Batch size mismatch (src). Current src batch size: {src.shape[0]}, expected batch size: {BATCH_SIZE}\"\n",
        "            # assert tgt.shape[0] == BATCH_SIZE, f\"Batch size mismatch (tgt). Current tgt batch size: {tgt.shape[0]}, expected batch size: {BATCH_SIZE}\"\n",
        "\n",
        "            tgt_input = tgt[:-1, :]\n",
        "\n",
        "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "            logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "            tgt_out = tgt[1:, :]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "            loss = loss / grad_accumulate_steps  # Normalize loss\n",
        "\n",
        "        scaler.scale(loss).backward()  # Scaled backpropagation\n",
        "\n",
        "        if (step + 1) % grad_accumulate_steps == 0:\n",
        "            scaler.step(optimizer)  # Update optimizer\n",
        "            scaler.update()  # Update scaler\n",
        "            optimizer.zero_grad()\n",
        "            torch.cuda.empty_cache()  # Clear GPU cache\n",
        "\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation in evaluation\n",
        "        for src, tgt in tqdm(val_dataloader, desc='Validating'):\n",
        "            src = src.to(DEVICE)\n",
        "            tgt = tgt.to(DEVICE)\n",
        "\n",
        "            tgt_input = tgt[:-1, :]\n",
        "\n",
        "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "            logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "            tgt_out = tgt[1:, :]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "            losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)"
      ],
      "metadata": {
        "id": "q2HD_u0XjT-Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print(epoch, train_loss, val_loss, epoch_time):\n",
        "    print(f\"\\n{'Epoch':<10}{'Train Loss':<20}{'Val Loss':<20}{'Epoch Time (s)':<20}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{epoch:<10}{train_loss:<20.3f}{val_loss:<20.3f}{epoch_time:<20.3f}\\n\")"
      ],
      "metadata": {
        "id": "ZNLjv4ncJWlN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 15\n",
        "GRAD_ACCUMULATE_STEPS = 2  # Adjust as per your requirement\n",
        "scaler = GradScaler()  # For mixed precision\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, train_dataloader, optimizer, scaler, GRAD_ACCUMULATE_STEPS)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, val_dataloader)\n",
        "    epoch_time = end_time - start_time\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    pretty_print(epoch, train_loss, val_loss, epoch_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLIdqcxhjU6G",
        "outputId": "06db014a-d48e-4a13-80bc-1a94f2525775"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:38<00:00, 16.32it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 26.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "1         4.195               7.124               38.296              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.70it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:05<00:00, 24.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "2         3.489               6.817               37.431              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.61it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 30.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "3         3.319               6.676               37.649              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:36<00:00, 16.89it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:05<00:00, 22.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "4         3.197               6.564               37.010              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.57it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 31.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "5         3.094               6.465               37.727              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.57it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:05<00:00, 21.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "6         3.006               6.417               37.723              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.77it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 29.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "7         2.929               6.340               37.274              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:38<00:00, 16.30it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 25.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "8         2.860               6.310               38.343              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.81it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 29.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "9         2.796               6.277               37.206              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:38<00:00, 16.10it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 29.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "10        2.738               6.285               38.826              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.88it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 25.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "11        2.683               6.251               37.035              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:38<00:00, 16.35it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 29.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "12        2.630               6.247               38.234              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:38<00:00, 16.42it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:05<00:00, 21.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "13        2.578               6.251               38.080              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.70it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:04<00:00, 30.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "14        2.527               6.289               37.433              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:37<00:00, 16.53it/s]\n",
            "Validating: 100%|██████████| 125/125 [00:05<00:00, 22.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch     Train Loss          Val Loss            Epoch Time (s)      \n",
            "----------------------------------------------------------------------\n",
            "15        2.477               6.302               37.832              \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Creating the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, NUM_EPOCHS+1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), val_losses, label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BPb-b3PNf3Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Model"
      ],
      "metadata": {
        "id": "E7R5N4GVIv4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(transformer, 'models/testing_the_nans.pt')"
      ],
      "metadata": {
        "id": "r8x4uYfp-c3t"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer = torch.load('models/30_epoch_emb_size_256_half_dataset.pt')"
      ],
      "metadata": {
        "id": "Q1bnLxnzoDkR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference and Decoding"
      ],
      "metadata": {
        "id": "A2dM_XixIx9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "def beam_search_decode(model, src, src_mask, max_len, start_symbol, beam_size):\n",
        "    # Initialize the beam with the start symbol and an initial score\n",
        "    initial_beam = (torch.tensor([start_symbol]), 0.0)  # (sequence, score)\n",
        "    beams = [initial_beam]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        for beam in beams:\n",
        "            # Expand the current beam\n",
        "            seq, score = beam\n",
        "            if seq[-1] == EOS_IDX:\n",
        "                # If the sequence is finished, pass it through\n",
        "                new_beams.append(beam)\n",
        "                continue\n",
        "\n",
        "            # Get probabilities of next words\n",
        "            # This part depends on your model architecture and may need modification\n",
        "            prob = get_prob_from_model(model, seq, src, src_mask)\n",
        "\n",
        "            # Choose top `beam_size` continuations\n",
        "            topk_prob, topk_indices = torch.topk(prob, beam_size)\n",
        "\n",
        "            for prob, word_idx in zip(topk_prob, topk_indices):\n",
        "                new_seq = torch.cat([seq, word_idx.view(1)])\n",
        "                new_score = score + torch.log(prob)\n",
        "                new_beams.append((new_seq, new_score))\n",
        "\n",
        "        # Sort all new beams and select the top `beam_size`\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "    # Choose the sequence with the highest score\n",
        "    best_seq, _ = max(beams, key=lambda x: x[1])\n",
        "    return best_seq\n",
        "\n",
        "\n",
        "def get_prob_from_model(model, seq, src, src_mask):\n",
        "    # Ensure the model is in evaluation mode, which turns off layers like dropout\n",
        "    model.eval()\n",
        "\n",
        "    # Encode the source sentence\n",
        "    memory = model.encode(src, src_mask)\n",
        "    # Add batch dimension to sequence for compatibility with the model\n",
        "    seq = seq.unsqueeze(1)\n",
        "\n",
        "    # Create a target mask for the sequence\n",
        "    tgt_mask = generate_square_subsequent_mask(seq.size(0)).to(DEVICE)\n",
        "\n",
        "    # Decode the sequence\n",
        "    output = model.decode(seq, memory, tgt_mask)\n",
        "\n",
        "    # Convert the output to probabilities using the generator\n",
        "    prob = model.generator(output[-1])\n",
        "\n",
        "    return torch.softmax(prob, dim=-1).squeeze(0)\n",
        "\n",
        "\n",
        "# actual function summarize a piece of text\n",
        "def summarize(model: torch.nn.Module, src_sentence: str, decoding_method: str = 'greedy'):\n",
        "    model.eval()\n",
        "    src = text_transform(src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    max_len = 50\n",
        "\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    if decoding_method == 'greedy':\n",
        "        tgt_tokens = greedy_decode(model, src, src_mask, max_len, start_symbol=BOS_IDX)\n",
        "\n",
        "\n",
        "    elif decoding_method == 'beam_search':\n",
        "        beam_size = 5\n",
        "        # Assuming beam_size is defined or passed as an argument\n",
        "        tgt_tokens = beam_search_decode(\n",
        "            model,\n",
        "            src,\n",
        "            src_mask,\n",
        "            max_len,\n",
        "            start_symbol=BOS_IDX,\n",
        "            beam_size=beam_size\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid decoding method. Choose 'greedy' or 'beam_search'.\")\n",
        "\n",
        "    tgt_tokens = tgt_tokens.flatten()\n",
        "    outputted_text = vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
        "    return \" \".join(outputted_text).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "jjCDeJyijZPr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"\n",
        "Playing computer games such as Angry Birds teaches children important life skills including concentration, resilience and problem solving, an academic has said. Professor Angela Mcfarlane, an education expert who will become head of training body the College of Teachers next month, said many games were complex and required deep learning and lateral thinking to solve them. Prof Mcfarlane said she herself had become 'hooked' on the Lemmings computer game, as well as Angry Birds, and said such games could have a place in the classroom provided they were used under supervision. Professor Angela Mcfarlane says computer games like Angry Birds can teach children valuable life-skills . Expert: Prof Mcfarlane says the games can help children learn problem solving, resilience and concentration . She said: 'There are many computer games that require quite deep learning to master the games. 'Some of that learning applies beyond games to wider life, such as concentration, problem solving, and resilience - important life skills. 'Anyone who has tried to play complex video games will know they are difficult.' Speaking to The Times, Prof Mcfarlane said she had developed an obsession with both Angry Birds and a precursor, Lemmings, because they had made her think and get her strategy right. The education expert, who has advised the government on educational technology, and who is currently writing a book, Authentic Learning for the Digital Generation, said computer games could be used in the classroom to good effect provided it was done properly. Prof Mcfarlane said even pre-school children could benefit from games, as long as they were supervised and not just given a phone to play with to keep them quiet. Prof Mcfarlane said she herself had become 'hooked' on a computer game called Lemmings, pictured . She said some games could teach children fine motor control, or help with vocabulary or simple maths, and taught skills such as resilience that could be applicable to real life. Next month Prof Mcfarlane, who began her career as a secondary school teacher and head of department, will become chief executive and registrar of the College of Teachers, which offers professional training to teachers and support staff.\n",
        "\"\"\"\n",
        "\n",
        "summarize(transformer, article, decoding_method='greedy')"
      ],
      "metadata": {
        "id": "xDfvF96Sk8-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1e267539-a057-4c7f-9fd4-5a4ffa89ef8c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The former former Court of the first time of the world 's mother . <unk> The former Court of her mother - old son 's mother 's mother . <unk> The couple have been charged with a ' I 'm not be ' <unk> The couple have been '\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def beam_search_decode(model, beam_size, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "#     sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "#     eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "#     # Precompute the encoder output and reuse it for every step\n",
        "#     encoder_output = model.encode(source, source_mask)\n",
        "#     # Initialize the decoder input with the sos token\n",
        "#     decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "#     # Create a candidate list\n",
        "#     candidates = [(decoder_initial_input, 1)]\n",
        "\n",
        "#     while True:\n",
        "\n",
        "#         # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
        "#         if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
        "#             break\n",
        "\n",
        "#         # Create a new list of candidates\n",
        "#         new_candidates = []\n",
        "\n",
        "#         for candidate, score in candidates:\n",
        "\n",
        "#             # Do not expand candidates that have reached the eos token\n",
        "#             if candidate[0][-1].item() == eos_idx:\n",
        "#                 continue\n",
        "\n",
        "#             # Build the candidate's mask\n",
        "#             candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
        "#             # calculate output\n",
        "#             out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
        "#             # get next token probabilities\n",
        "#             prob = model.project(out[:, -1])\n",
        "#             # get the top k candidates\n",
        "#             topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
        "#             for i in range(beam_size):\n",
        "#                 # for each of the top k candidates, get the token and its probability\n",
        "#                 token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
        "#                 token_prob = topk_prob[0][i].item()\n",
        "#                 # create a new candidate by appending the token to the current candidate\n",
        "#                 new_candidate = torch.cat([candidate, token], dim=1)\n",
        "#                 # We sum the log probabilities because the probabilities are in log space\n",
        "#                 new_candidates.append((new_candidate, score + token_prob))\n",
        "\n",
        "#         # Sort the new candidates by their score\n",
        "#         candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "#         # Keep only the top k candidates\n",
        "#         candidates = candidates[:beam_size]\n",
        "\n",
        "#         # If all the candidates have reached the eos token, stop\n",
        "#         if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
        "#             break\n",
        "\n",
        "#     # Return the best candidate\n",
        "#     return candidates[0][0].squeeze()"
      ],
      "metadata": {
        "id": "nTbN7VdADlAu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_article in test_articles[:10]:\n",
        "\n",
        "    summary = summarize(transformer, test_article, decoding_method='greedy')\n",
        "    print(test_article)\n",
        "    print(summary, end='\\n\\n')"
      ],
      "metadata": {
        "id": "rIfl6lcQ8yq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e89f6c-daa9-4424-ef60-ca510a1159bb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mother and daughter who survived a tragic car accident this week, which saw three children die, have been reunited. Aluel Manyang was moved from the intensive care unit at the Royal Children's about 5.15pm on Friday, and greeted her distraught mother, Akon Goode, with a 'big hug', her father said. 'She didn't believe that her mum was still alive,' Joseph Manyang said, according to the Herald Sun. Scroll down for videos . Aueel Manyang, pictured here as a baby with her mother Akon Guode, believes her three siblings who died in the crash at a Melbourne lake were eaten by crocodiles in the water . Ms Guode visited her daughter for the first time but did not stay the night in the hospital. Mr Manyang said his daughter was expected to make a '100 per cent' recovery and she should be allowed to go home within four days. The five-year-old girl who survived when a car driven by her mother plunged into a lake believes her three siblings who died in the crash were eaten by crocodiles. Aluel Manyang believes her younger sister and two brothers died when they were taken by a crocodile because the children associate the giant reptiles with water. Her siblings - one-year-old brother Bol and four-year-old twins Madit and Anger - died after their mother Akon Guode crashed her 4WD into a lake at Wyndham Vale in Melbourne's outer west on Wednesday. The crash on Wednesday killed four-year-old twins Madit and Anger (pictured) and their one-year-old brother Bol when the 4WD plunged into a lake . The children's father, Joseph Tito Manyang, says five-year-old Awel remains in a serious condition at the Royal Children's Hospital but she remembers the accident. 'Always when they see water they think of crocodiles, so that's what she said. She said they'd fallen into the water and there were crocodiles eating my younger brothers and sister,' Mr Manyang said. 'I went to see her yesterday and she was happy to see me. She knew me when I asked: 'Do you know me?' She said, 'Yeah, you're Daddy'.' Mr Manyang's partner and mother of the children, Akon Guode, was released from police custody on Thursday night after homicide detectives questioned her over the crash. She told him she was feeling 'very dizzy' before the accident and remembers her children crying out. One-year-old Bol (left) and four-year-old twins Madit and Anger (right) all died after the 4WD they were in crashed into a lake at Wyndham Vale in Melbourne's outer west just before 4pm on Wednesday . Ms Guode has been sent to stay with a relative after portraits of the deceased children and memories of them running around the family home became too much. 'She's very shocked,' Mr Manyang said. 'She was crying every 10 minutes. She remembers everything at home and how the kids were walking around and playing.' Mr Manyang says his partner told him she 'didn't feel herself' as she was driving and can only remember parts of the aftermath of the crash, The Age reports. But she does recall being in the water and hearing her surviving five-year-old daughter Awel saying 'Mama, the children are falling under the water'. Mr Manyang was struggling with the news his three young children had died. 'The feeling is very hard for me,' he said. 'My kids, they were very good ... always, they were happy. Especially the twins. It is a big shock to me to get this information.' He revealed Ms Guode called him at 2.16pm on Wednesday afternoon but by the time he returned the call there was no answer. 'Bol, is still shy, is one year and a half, always we play together, that's what has happened, but it is a big shock for me to get this information. It has happened to me, to lose three children at once,' he told SBS radio. The children's father, Joseph Tito Manyang (second from right), was hugged by a mourner at the crash site at Wyndham Vale in Melbourne's outer west . It comes as Mr Manyang defended the children's mother earlier on Thursday saying he didn't believe she caused the crash deliberately and said she was extremely distressed. 'She is a very good mother,' he told the Herald Sun. 'She loved the kids. She took care of the kids. I don't think she planned to do anything. 'I'm still believing she's innocent. 'My message to the community - because there is a lot of talking around, there is untruths - what I can tell them is they have to wait for the report from the police investigation, that will be the truth.' He  made an emotional visit to the crash site on Thursday where floral tributes were starting to build up from the Sudanese community who knew the family and from strangers. Ms Guode, who is the mother of seven children, came to Melbourne from South Sudan to escape war after her first husband died. The children's mother, Akorn Manang, was behind the wheel of grey 2005 Toyota Kluger when it plunged into the lake on Manor Lakes Boulevard . Police have retrieved the four-wheel-drive after it was plunged into the Wyndham Vale on Wednesday . Mourners pray at a makeshift shrine at the scene where a 4WD was submerged in Lake Gladman . Mr Manyang's 16-year-old niece, Amani Alier, said the family were trying to console him following the devastating news. 'He's shattered, he couldn't stop crying, his nose was bleeding,' she said. 'He just dropped when he saw a picture of his son on the wall. It's so hard for him to deal with. He loved those kids.' It follows as the four-wheel-drive was removed from the lake on Thursday afternoon. Thomas Kok, the cousin of the children's father, who spent the night at the children's hospital with Awel, visited the crash site on Thursday morning to pay tribute to the children. 'It's a shock for everyone,' Mr Kok said, according to the Herald Sun. 'We spent all night at hospital until 3 in the morning.' It comes as the tight-knit Sudanese community and nearby residents started leaving floral tributes for the three children. 'As soon as I saw the community coming towards this scene where the incident's happened, it's making me feel like people care,' Mr Kok said. The tight-knit Sudanese community and nearby residents started leaving floral tributes for the three children . The 2005 Toyota Kluger is expected to be removed from the lake late on Thursday morning . The family with seven children moved from South Sudan to Australia in 2008 . Thomas Kok, the cousin of the children's father, who spent the night at the children's hospital with Awel, visited the crash site on Thursday morning to pay tribute to the children . Police have confirmed the mother is assisting the homicide squad with their investigation as they appealed for members of the Sudanese community to come forward if they knew anything about what was going on in the family's life. 'We need to understand what led to this, we need to understand the background of this family, we need to understand what was happening in their lives,' Superintendent Stuart Bateson said. 'Hopefully this will lead to a greater understanding of what led to this incident. We appeal especially to the Sudanese community who might know what was happening in this family's life, what their movements were before this tragedy.' A witness Michelle relived the tragic moment the car ploughed into the lake on Thursday morning. 'It is still in my mind now, every time you close your eyes you just see someone so little,' she told Nine News in tears. 'My heart goes out to the family and to everybody who witnessed it. 'Those rescuers, those workers, when it was winding down it was just devastating seeing them sitting in the gutter.' Michelle told 3AW late on Wednesday that her friend Travis rescued one of the young boys from the vehicle. 'Travis jumped into the water and smashed the back windscreen and got one little boy out. (Emergency workers) then started working on the child,' she said. 'I only saw three get taken out. I believe they were travelling down the boulevard...and they've just gone over and into the lake.' Emotions were still raw on Thursday morning for neighbour Michelle who witnessed the grey 2005 Toyota Kluger plunging into the lake on Manor Lakes Boulevard . The search and rescue team remained at the scene to try and pull the car from the water, which is reportedly about 20 to 30 metres from shore . Locals are shocked as to how the car was able to get into the centre of the lake as it is 'pretty far from the road' Detective from the Homicide Squad and the Major Collision Investigation Unit have spoken to some witnesses but they are appealing for anyone with information to come forward. It is not yet know if it was a deliberate act or an accident, according to police. It's believed the grey 2005 Toyota Kluger 4WD was travelling towards Pedder St and Minindee Road when the incident occurred. Police particularly wish to speak to anyone who saw the car between 3.30 and 3.45pm. Witnesses or anyone with further information about the incident should contact Crime Stoppers on 1800 333 000. A mini shrine for the three young children grew on Thursday as families visited the area to pay their respects . Two young girls carried flowers to the lake where three three died when the car they were in crashed . Victoria police said that the exact circumstances of how the car went into the lake are yet to be determined . A resident named Michelle told 3AW that her neighbour broke open the back windscreen of the 4WD to save the children . Victoria Ambulance said the children were taken to The Royal Children's hospital via air ambulance . Further tests will be conducted in order to determine whether the car had any mechanical defects .\n",
            " Police say they were found guilty of the home in the man . <unk> She was found guilty of the home in the home of her home . <unk> She was found guilty to the man and had been charged with a ' <unk> She was found in the\n",
            "\n",
            "The mother of a two-year-old boy who fell 10 feet into a cheetah enclosure could face child endangerment charges after witnesses claim she was dangling him over the edge. Visitors at Cleveland Metroparks Zoo in Ohio heard a scream as the toddler tumbled into the pit at 3pm on Saturday. His parents jumped in and pulled him to safety before paramedics arrived to treat the boy for a leg injury. Though the felines appeared to avoid the child, Dr Christopher Kuhar, executive director at the zoo, said there is a strong chance charges will be pressed on Monday. Scroll down for video . A mother was holding the two-year-old boy and another child when the toddler slipped and fell into the cheetah exhibit at the Cleveland Metroparks Zoo (file photo of cheetahs at the Cleveland zoo) The boy was rescued by his parents from the pit (pictured) before firefighters and paramedics arrived on the scene. He suffered from minor bumps and bruises and was listed in stable condition at the hospital . He told CNN: 'Unfortunately, we have a number of eyewitness accounts that point to the strong likelihood that the child was dangled over the rail.' According to witnesses the mother was holding him and another child when he fell. Michael Lurie and his family were at the Cheetah exhibit when they heard the child scream. 'You saw how far the drop was and you just couldn't believe the kid didn't hurt himself from falling down on the ground,' Lurie told WKYC. 'I was just shocked,' he said. 'I didn't understand how the parents let the kid go over the thing.' The cheetahs did not approach the boy or his parents while in the pit, according to zoo officials. Zoo visitor Terra Lurie believes the boy was not approached by the fast feline because they were frightened. 'I think they were just curious as to what was going on and why somebody was in the pen with them,' she said. 'It's not every day that somebody is just in the pen with them.' 'And everyone else is screaming and they probably got scared.' The exhibit was closed following the child's fall. Zoo visitor Michael Lurie was at the cheetah exhibit when he heard the child scream. He said he was 'shocked' and 'didn't understand how the parents let the kid' go over the railing and into the pit . Cleveland Metroparks Zoo plans to press child endangering charges against the child's mother (above file photo of visitors at the Cleveland zoo)\n",
            " The man was found in the man , was arrested on the home . <unk> The couple were found dead in the man , was a year - old son 's body . <unk> The man was found dead in the home in the home . <unk> The couple\n",
            "\n",
            "The pups came from as far away as New Jersey and Tennessee in hopes of landing a new nickname: 'Beautiful Bulldog.' They were all defeated by a native who likes eating snow and watching turtles. A 2-year-old dog from Des Moines named Tank won the 36th annual Beautiful Bulldog contest Sunday at Drake University. Scroll down for video . Winner: Tank, a 2-year-old bulldog from Iowa won Drake University's 36th annual 'Beautiful Bulldog' contest Sunday . A real beauty: Tank, who enjoys eating snow and watching turtles, will now serve as mascot of this year's Drake Relays . Tank received top honors as well as a crown and cape. He will appear before more than 16,000 fans -- or, royal subjects -- at the university's Drake Relays to be honored as mascot of the event, which will be held from Thursday through Saturday, according to the contest's website. The tongue-in-cheek beauty pageant, which featured 50 dogs, is the kickoff event for the Drake Relays track meet. 'He's funny,' said Tank's owner, Duane Smith. 'He's a real good one.' Pageant organizers narrowed a pool of more than 100 hopeful pups by a lottery held last month. Owner: Here, Tank enjoys a rub from his owner, Duane Smith, after winning the 36th annual contest . Judges weren't looking for beauty though. They wanted to see the slobber, drool and bulging, bloodshot eyes synonymous with English bulldogs. They got all that and more from Tank — who now willingly shares his house with a Pomeranian and some turtles after Smith found him on Craigslist a year ago. Should Tank be unable to fulfill his duties as the Drake Relays mascot, fellow Des Moines pup Steve will step in. Steve was second even though he was initially so shy about the makeshift catwalk set up on Drake's basketball court that his owner had to pick him up and plunk him down, much to the delight of the few thousand spectators on hand. Pageant: The tongue-in-cheek beauty pageant, which featured 50 dogs, is the kickoff event for the Drake Relays track meet . Bex in a tutu: Ronnie Sussman, of Union, N.J., walks her dog Bex across the stage during the contest as part of the Drake Relays . There also was a 'Best Dressed' winner in Linus the Lovebug — who had to be dragged around in a wagon because of arthritic legs — and the congeniality award went to a dog named Princess Mabel. If there was an award for driving the furthest to enter the contest it would've gone to Ronnie Sussman and her dog Bex, who drove 17 hours from Union, New Jersey. Sussman and Bex will go home empty-handed, but she said the trip was more than worth it. 'This is just like a bucket list item of life for me,' Sussman said.\n",
            " The former State - year - old was arrested on the world 's first time . <unk> The former PM 's first time of the first time of the world 's side . <unk> The pair were found in the world 's first time . \n",
            "\n",
            "Doctors have called for under-fours to be given free vitamins after a rise in the number of cases of rickets due to a lack of exposure to sunlight. The country's chief medical officer Dame Sally Davies is said to be concerned at the number of children suffering from the condition, which is caused by a deficiency in vitamin D. The disease, a scourge of Victorian Britain, was virtually eradicated after the Second World War but is returning as more and more youngsters are used to staying indoors playing video games than going outside. Scroll down for video . Dame Sally Davies, the country's chief medical officer, who has ordered a cost review into giving all under-fours free vitamin D supplements . Now, it has been reported that Professor Davies has ordered the National Institute for Health and Clinical Excellence (Nice) to review the cost of providing vitamin supplements to all children under the age of four, in a bid to reverse the trend. The move is being supported by one of Britain's leading experts on vitamin D deficiency at University College Hospital London Alastair Sutcliffe, who has spoken about an 'epidemic' of cases due to a lack of sun exposure and overuse of sunscreen. He told the Sunday Times: 'Nothing is free but the cost of the ill-effects of deficiency, such as rickets and anaemia from families not providing children with these supplements is greater for the NHS. 'Sunblock is so powerful it does work but you end up with no exposure to the sun. 'The outcome is that you are blocking out sunshine and you have a secondary effect of reduced exposure to sunshine which the human race needs.' Rickets is caused by a deficiency in vitamin D and causes bone deformities such as bowed legs, pictured, and a curvature of the spine . Rickets, which is also known as soft bones, can cause deformities such as bowed legs and a curvature of the spine. During the war children were given food supplements such as cod liver oil, but this practice stopped in the 1950s. An estimated 40 per cent of children are estimated to have vitamin D levels below the recommended amount. Figures from the NHS show there were 833 hospital admissions for children suffering from the condition  in the financial year 2012-13. Professor Davies has previously spoken out about the advantages of giving free vitamins to young children. A scheme has already been set up in deprived areas of Birmingham handing out supplements, which has halved the number of cases of rickets and other deficiency problems. She said: 'We are offering these vitamins to vulnerable children and the take-up is low, but many children not in these communities need them too.' The NHS already recommends all youngsters aged six months to five years take the vitamins, but parents must pay for them unless they are part of the NHS’s means-tested Healthy Start scheme. Professor Davies said the UK’s record on children’s health used to be ‘one of the best but we are now the worst’. Possible causes include lifestyle behaviours in pregnancy such as smoking, and poor care in infancy. Rickets, or soft and deformed bones, was first noted by physicians in ancient Rome but was not linked with lack of vitamin D until the start of the 20th century. It was common in Victorian times because of lack of access to sunlight – which the body needs to make vitamin D – and poor diets. It mostly disappeared in the West during the 1940s thanks to the fortification of foods such as margarine, and children were also routinely given cod liver oil. Rickets is still a major problem in third-world countries. However, UK cases have been rising in the past 15 years, from 183 in 1996 to 762 in 2011. Experts believe this is partly because children are eating less fish and eggs than in the past. They also blame extensive use of sunscreen, and children spending more time indoors.\n",
            " The company is the most of the U.S. minister , and other people are being used to be used by the UK . <unk> The couple are now also also used to be used by the same time . \n",
            "\n",
            "This is the shocking moment a toddler was left hanging by her neck as she tried to climb out of her cot - and was only saved because her mother was watching her on the baby monitor. Ophelia Conant had managed to crawl backwards through a gap in the cot at her home in Holmer Green, Buckinghamshire, but was left hanging mid air by her neck. The furniture boss who supplied the beds, Phillip Dickens, who was described as the 'king of DIY' was today fined £50,000 and given a suspended prison sentence. Ophelia Conant managed to crawl backwards out of her cot and get trapped between the horizontal handrail . The 19-month-old was left dangling mid air by her neck and was only saved because her mother had been watching her on the baby monitor . The 19-month-old was only saved when her mother, Louise Conant, happened to be watching the incident unfold on a baby monitor. Days later a second toddler was found dangling from the £450 bed with his forehead jammed against the horizontal handrail. Both mothers had purchased the Nutkin three-drawer cotbeds from Dicken's company, Baumhaus Ltd, which had the beds produced in China and imported back into Britain for distribution. Amersham Crown Court heard Mrs Conant had put Ophelia in the cotbed for her lunchtime sleep on April 16, 2013, when she saw her getting into difficulty via the video monitoring system. She rushed upstairs to find her daughter hanging by her neck, which had become lodged in a gap between the end of the bed and a horizontal handrail. Judge Karen Holt told Dickens: 'Seeing her daughter on the baby video monitor screen hanging by her neck was, using her words, beyond description and completely traumatic to her. 'One can only imagine the horror that mother must have felt. In her view, it is without doubt the fact she had a video monitor that saved her daughter's life.' She added: 'This is a serious offence because it came through your negligence. It could have caused the loss of life of babies. 'The crown's case against the company and you was, with your negligence, this unsafe product was put on the market. 'Clearly, you did not act with all due diligence, or take all reasonable precautions to make sure such a situation would not occur. 'The product itself was dealing with the most vulnerable people in society, and I know that you accept that.' Louise Conant with her daughter Ophelia today after what she described as a 'traumatic' experience . Judge Holt said the design of the cot 'could have been fatal' and added that although Dickens and the company had been 'naive and negligent', there was 'very strong mitigation' on their behalf. Nine days after Ophelia's incident, Deborah Turner, from Northamptonshire found her son dangling on the outside of the cot with his forehead jammed against the horizontal handrail. 'It took both her and her mother to lift her son back through the gap,' said Judge Holt. Mrs Conant complained to Trading Standards at Buckinghamshire County Council and officers took away the cotbed before confronting Baumhaus. They were shown a copy of the video of Ophelia and immediately recalled all the cots it had sent out around the country. The court was told no health and safety tests were carried out in the UK, with the only quality control being made in the Chinese factory, which did not comply with British safety standards. Following test reports and an offer from Dickens to give Mrs Conant a part-refund, Baumhaus was served a product recall notice in July 2013, which recovered 93 per cent of the 212 cotbeds sold. The following month Trading Standards received three lab test reports which identified failures, including test parameters and clauses relating to strength of certain parts, finger, head and neck entrapment, and BSI standards markings. Thames Valley Police visited Baumhaus in October and seized computer equipment. A month later Dickens was interviewed under caution during which he confirmed the company had designed the Nutkin cotbed, which was produced in China and imported back into Britain for distribution. The court heard the prototype had been tested and passed in 2010 but the product placed on the market in 2013 had different dimensions and Dickens had no proper quality control in this country. In mitigation Sunyana Sharma told the court neither Dickens nor Baumhaus set out to harm children with their cotbed, and the company accepted that it failed in not having proper quality control. She added that the company had acted quickly to remove the product from distribution and done all it could to trace the remaining cots, all of which were accepted by Judge Holt. Phillip Dickens, company director of furniture supplier Baumhaus Ltd, which distributed the beds, was today given a suspended prison sentence for placing an unsafe product on the market . Judge Holt described this as a serious offence but said the court acknowledged naivety and negligence on the part of the defendant, who at the time had no manufacturing experience, kept no technical product notes, and relied on an external company for testing to assess compliance with relevant British Standards. Dickens, described as 'the king of DIY' on a profile on the Baumhaus website, entered pleas on the company's behalf at an earlier hearing, admitting two counts of placing an unsafe product on the market between January 2010 and July 2013. The 38-year-old, of Burnley, Lancashire, was given a three month prison sentence suspended for 12 months. He also pleaded guilty to a similar charge relating to his company, which is based near Bicester, Oxfordshire, and ordered to pay £500 to the two mothers. Baumhaus was given a £12,000 fine and Dickens and his company also face prosecution costs of £35,653. Judge Holt said she felt able to suspend the jail sentence because of his lack of intent to cause injury in supplying the potentially deadly cots to the market. She added: 'It is very difficult to put a figure on the clear distress that both these ladies suffered.' In a statement read through solicitor Tim Healy, Dickens and Baumhaus said they 'sincerely regretted' any distress caused to the families. Speaking outside court Dickens' solicitor, Mr Healy, said: 'Baumhaus Limited and Philip Dickens wish to express their sincere regret for any distress caused to the families effected. 'Health and safety remains of paramount concern to Mr Dickens and the company.' Speaking after the hearing Mrs Conant said the court case brought an element of closure to the nightmare experience of seeing her daughter trapped by her neck in her cot. She said: 'I bought what I thought was the best for Ophelia - a premium quality cot bed. But it turned out to be a living hell. Standards Officers Rebecca Kaya (right) and Kirit Vadia (left) examine the Nutkin three-drawer cotbed . 'For a long time afterwards it gave me nightmares, and it made me nervous about putting Ophelia to bed. 'The video monitor definitely saved her life. I'd never have thought to buy one, but a friend suggested it. Now I'd urge every parent to consider a video monitor for their child.' Trading Standards Officer Rebecca Kaya, who led the investigation, said the case demonstrated the department's commitment to protecting families from products posing a safety risk, and was an example of the tough measures they were prepared to take. She said: 'This case highlights the need to be very careful buying online. Before clicking the \"buy\" button, make sure you have a manufacturer's code, product code and full contact details of the retailer, including a cancellation process in case you need to return the product, or there is a safety problem.' Ms Kaya added that they believe there are still 12 cotbeds out there that weren't reached during the product recall.\n",
            " The couple were found guilty of her husband , who was arrested on her husband , she was found in her husband , and her husband , but she was found . <unk> She was found guilty to be a ' a ' <unk> She was found in her\n",
            "\n",
            "Model Karlie Kloss is spreading her passion for computer coding and empowering young women by teaming up with the Flatiron School in New York to offer teen girls around the country scholarships to the academy's software engineering program this summer. The 22-year-old's Kode With Karlie scholarships will provide 20 girls between the ages of 13 and 18 free tuition to the same two-week coding course she took last year at the Flatiron Pre-College Academy. 'Code is only going to continue to play a major role in defining our future,' she says in a video promoting Kode With Karlie. 'I think it's crucial that young women learn to code as early as possible to ensure that we [...] have a voice and a stake in what the world looks like.' Scroll down for video . Role model: Karlie Kloss can be seen flexing her muscles in a promotional video for her Kode With Karlie scholarship program, which will provide 20 young girls free tuition for a computer coding course this summer . Team player: The 22-year-old model was filmed working with other students at the Flatiron School in New York, where her scholarship recipients will be taking a two week software engineering program . Karlie is donating $20,000 of her own money for the classes, which cost $2,000 per student, while the Flatiron school has offered to match her contribution. The blonde beauty, who describes herself as a 'model, ballerina and cookie expert' in the clip, explains that she learned how 'creative coding truly is' when she studied it last summer. 'Similar to dance and fashion, coding is a form of self-expression,' she says. 'It's a way to turn a cool, innovative idea into a product, website, app, tool or experience.' Karlie says it was this revelation that inspired her to share her passion with other girls across the country. Major revelation: Karlie attended the coding course, which costs $2,000 per student, last year . Deep in thought: The former Victoria's Secret Angel can be seen working on a computer programming project with other students . Completed project: Karlie and other students watched their hard work pay off as their creation took flight . After the course, her 20 scholarship recipients will understand the 'building blocks of coding' and have already built and deployed their own app to the web. In order to apply for the scholarship, female students are asked to film a 60 second video explaining how they plan to use their 'coding super powers' after they complete the course. Karlie took to social media yesterday to promote the opportunity, tweeting: 'Who runs the world? CODERS! Apply for the #KodeWithKarlie Scholarship @FlatironSchool for your chance to learn code! [sic]' And it's clear that Karlie, who also co-founded Karlie’s Kookies with chef and owner of Milk Bar Christina Tosi, takes her entrepreneurial roles and studies seriously. Stellar student: Karlie can be seen working with other students at the software engineering class last summer . Combined interests: The blonde beauty can be seen speaking on a panel about how technology revolutionized fashion week at SXSW in Austin, Texas . In February, it was announced that she was hanging up her Angel wings and leaving Victoria's Secret to focus on other business opportunities, as well as her classes at New York University. The model will be starting college at NYU's Gallatin School of Individualized Study in the fall, where she will study computer coding. Karlie, who covered the summer issue of Porter, told the magazine that she 'definitely could not get a job as a coder yet'. American girl: Karlie, who covers the summer issue of Porter, told the magazine she is 'fascinated' by computer coding . Catwalk queen: Karlie can be seen strutting down the runway at the Tom Ford show (L) and the Atelier Versace haute couture show (R) earlier this year . 'But it is something I am fascinated by, even if it is not the profession I choose,' she added. Karlie continued: 'Simply having the comprehension of the way that it works, having the ability to speak that language and to be able to not only comprehend it, but write and build something – the world is built on code.' And Karlie has no problem blending her two worlds together. Last month, she spoke on a panel about how technology revolutionized fashion week at SXSW in Austin, Texas.\n",
            " The couple has been found in the UK 's body of the UK . <unk> She has been found by the first time of the UK and is ' <unk> But the UK is ' I 'm not be ' <unk> She said it is ' and is '\n",
            "\n",
            "The family of a former art teacher who died from lung cancer after years of pinning pupils' work to classroom walls lined with asbestos has taken legal action against the local council. Jennifer Barnett worked Archway School in Stroud, Gloucestershire, between 1980 and 1997, when she left teaching to have her fourth child. The 60-year-old died last September, 14 months after she was diagnosed with mesothelioma - a cancer whose most common cause is asbestos. Loved: Jennifer Barnett worked Archway School in Stroud, Gloucestershire, between 1980 and 1997, when she left teaching to have her fourth child. Above, the mother-of-four with her husband and children . Her husband, Nigel, told an inquest into her death in January that his wife had to cut sheets of asbestos while working on a farm in her 20s. 'She then became an art teacher and from 1977 until 1995 worked at various schools, often hanging paintings on walls containing asbestos,' he added. The coroner, Katy Skerrett, recorded a verdict of death as a result of industrial disease. A post-mortem found a large tumour in her right lung. Ms Skerrett said: 'It is clear that there was sufficient exposure to asbestos in her occupation for me to reach a conclusion that this lady died from an industrial disease.' Mr Barnett and his family believe the asbestos in the school is to blame for Mrs Barnett's death and are now taking legal action against Gloucestershire County Council. In a statement released this week, Mr Barnett said that during her time as a teacher, his wife also did clay modelling with pupils and the cupboard where the clay was stored also contained asbestos. He said: 'Jen was a fantastic wife and a wonderful mother to our four children. She was so precious to us and it is hard to believe she's gone. 'Prior to her illness, she was a very fit and healthy 60-year old who enjoyed playing tennis in her spare time and was a dedicated and extremely talented artist who continued to work right up until her death. 'Our youngest daughter was only 18 when her mum passed away and was just about to start university. 'The whole family misses her dreadfully and we are all shocked and devastated that her life was cut short so suddenly.' Mrs Barnett worked at Archway School, where she was known as Miss Shonk, as an art teacher before being promoted head of the department. She left teaching aged 42 to have her youngest daughter, but continued to work as an artist. Her family said she remained positive during 'gruelling' chemotherapy treatment, but died the day after celebrating her 30th wedding anniversary, in September 2014. Dedicated: For 17 years, Mrs Barnett worked at Archway School, in Stroud, Gloucestershire, pictured, where she was known as Miss Shonk, as an art teacher before being promoted head of the department . Mr Barnett said: 'I'll never forget the consultant at the hospital asking Jen if she'd ever been exposed to asbestos and her saying a definite \"yes\" because she knew a lot of asbestos was previously used in school buildings. 'Jen fought the cancer bravely and remained positive throughout gruelling sessions of chemotherapy. 'The day before she died, she even managed to come to the little village church where we were married to celebrate our 30th wedding anniversary and we had a very special day together as a family. 'Photographs taken that day show her smiling and we will always treasure them.' Mr Barnett, from Painswick, Gloucestershire, has instructed specialist asbestos disease lawyers at Novum Law to investigate his wife's working conditions at the school. He added: 'I am hoping former teachers or ex-pupils will come forward who may have some knowledge about the asbestos ceiling tiles at Archway School or know of any other asbestos products or materials that were used there.' Helen Grady, an asbestos disease solicitor at Novum Law said: 'It is alarming that we are seeing more and more cases involving teachers. 'Asbestos was widely used in the UK as a building material for fireproofing and insulation from the 1950s up until 1985, when most types were banned. 'It was banned completely from new buildings in 1999 but, sadly the damage had already been done for thousands of victims who have paid the ultimate price for going to work every day. 'We hope anyone who may have attended Archway or worked there remembers Jen and can shed more light on how she got exposed.' Archway headteacher Colin Belford said staff were saddened by their former colleague's death, and were not aware of the issues raised by the coroner. He said: 'Staff who worked with her at Archway spoke of her with great affection and admiration. 'I can say that all of the building work which has taken place at Archway in recent years is fully compliant with modern building regulations. 'I am not aware of any current risk to staff or students but will ask the local authority for its assurance on this matter.' Phil Ashbee-Dobbins, Gloucestershire County Council asbestos administration officer, said: 'Buildings built in the 1960s and 1970s, and even up until the late 90s often have asbestos in them. 'It doesn't pose a risk unless it is disturbed, and we help schools under local authority control keep their own accurate records and to make sure it is treated properly and safely. 'Archway School has been extensively renovated in recent years. In addition, regular asbestos audits are completed as part of standard health and safety procedures. 'While we are unable to comment on this specific case, we would like to reassure parents, pupils and teachers that current guidance and practices in our county's schools appropriately manage the risks associated with asbestos.'\n",
            " The couple were found guilty of her husband , who was arrested on her husband , she was found in her husband , but she was a ' <unk> She was found in the dog and her husband and her husband and her husband , who was a '\n",
            "\n",
            "Bradley Dew has been jailed for mugging his deaf friend to steal his mobile phone and a £10 note . A thug has been jailed after he mugged his deaf friend to steal his mobile phone and a £10 note. Bradley Dew, 26, had been drinking with his friend at a pub in Faversham in Kent earlier that day and had borrowed his mobile. But later in the evening Dew returned to his friend's home, claiming he had lost the mobile on a train after returning from Herne Bay. As the friend said he was about to go out with his girlfriend, Dew pushed him back into a wall before striking him across the face. He then stole £10 out of his wallet. Dew, of Faversham, then fled the scene, and was found by police hiding in the toilets of a pub close to his friend's home hours later on March 29 last year. Officers said he was likely to have been left feeling vulnerable after being mugged in his own home. Dew denied stealing from his friend, and told officers it would have been 'like robbing my brother.' However he was found guilty of robbery and assault, following a trial held at Maidstone Crown Court. Detective Constable Kathryn Lumsden-Earle, of Kent Police, said: 'Dew took advantage of a so-called friend's generosity, taking his phone and then never returning it. 'He then acted like a bully by pushing and hitting his vulnerable victim before stealing money from his wallet. 'He did all of this in the victim's own home, which no doubt left him very frightened and scared for his safety. 'Time behind bars just might give him that opportunity to reflect on what he did that day, but more so the impact it had on someone he knew was vulnerable and who viewed him as a friend.' Dew, 26, was found guilty of robbery and assault by a jury at Maidstone Crown Court (pictured)\n",
            " Police say he was ' I had been charged with his wife . <unk> He was ' I had been ' and ' <unk> She was ' I 'm not been charged with her ' <unk> He was ' and ' and ' I 'm not ' \n",
            "\n",
            "Victor Moses has returned to Chelsea after being ruled out for the season. The winger has been on loan at Stoke but damaged his hamstring in the 1-1 draw at West Ham United on Saturday and is expected to be out for six weeks. The 24-year-old has gone back to parent club Chelsea for treatment following the results of a scan. Stoke are still keen to strike a permanent deal for Moses in the summer. Victor Moses has returned to parent club Chelsea for treatment after injuring a hamstring against West Ham . Stoke are still interested in tying up a permanent move for the Nigeria international after a successful loan . Meanwhile, the Potters have opened talks with Ligue 1 side Evian over the possible transfer of defender Daniel Wass - who has been watched by Chelsea, Liverpool and Newcastle over the past year. The 25-year-old Denmark international is available for around £3.5million and is keen to try his luck in the Premier League, despite interest from the likes of Inter Milan and Schalke. Hughes has been given licence to spend this summer with players including Moses, Manchester United forward Javier Hernandez and Sunderland midfielder Lee Cattermole on his wishlist. Stoke scouts have also been monitoring Sporting Lisbon pair Islam Slimani and Andre Carrillo. Stoke have entered the chase for Evian full-back Daniel Wass (2nd left), who has been watched by Chelsea .\n",
            " The club will be a £ 1 - 0 in the Premier League . <unk> The club will be a £ 1 - year - year - year - old . <unk> The club will be a £ 1 - year - old . \n",
            "\n",
            "Real Madrid's Champions League victory over city rivals Atletico on Wednesday night marked the eighth match Gareth Bale has not started this season... and Real have won every single one. The Welshman has received some criticism from sections of the Bernabeu support despite a bright start in Madrid, and these remarkable stats will do nothing to help his popularity at the club. The £86million man has missed out on just eight starts in Real Madrid's 50 matches this term, but his team-mates have picked up a 100 per cent win record when he has been away, scoring 25 and conceding just one goal. Gareth Bale has not started eight of Real Madrid's matches this season, through injury and omission . His latest injury, suffered against Malaga, meant he missed Wednesday night's Champions League game . Bale's time on the sidelines has coincided with huge matches against Barcelona, two Champions League ties with Liverpool (in which he entered the fray as a 62nd-minute substitute at the Bernabeu), and Wednesday night's European quarter-final against Atletico... but it seems the rest of the Real squad are just fine without him. In the first half of the season, Bale suffered a buttock muscle injury while on international duty with Wales, and he missed La Liga matches against Levante (5-0 win), Barcelona (3-1 win), Granada (4-0 win) and a 3-0 Champions League victory against Liverpool. He was in contention for the return leg against Liverpool at the Bernabeu, but did not start the match, instead coming on as a second-half substitute. Real were 1-0 up when he came on, and the score remained the same until the final whistle. In December, Madrid were already leading 4-1 against third-tier side Cornella in their Copa del Rey tie heading into the second leg, and Carlo Ancelotti opted to rest some of his biggest names. Cristiano Ronaldo, Karim Benzema, Toni Kroos, Sergio Ramos and Bale were all among the stars watching on from the sidelines as Los Blancos recorded a comfortable 5-0 win. That's six games without Bale in the starting line-up, and six wins for Real. Moving into 2015, Bale played a major role in the team's start to the year, scoring 12 goals between his return to the side against Celta Vigo and his omission against Eibar on April 11. Bale was sidelined for Real's 3-1 win over Barcelona, in which Cristiano Ronaldo scored a penalty . Karim Benzema scored in a 3-0 win against Liverpool at Anfield in October... Bale was injured for the game . Ancelotti stated in the build-up to the match that he had decided to rest the Welshman, while the team went on to win 3-0 against their struggling opposition. Bale again returned the side for two matches, a goalless draw against Atletico Madrid in the Champions League and a win against Malaga in La Liga, but suffered a calf injury in the early stages of the latter domestic clash. Instead of taking to the field in the crucial European match on Wednesday night, Bale was resigned to watching the match from home, where he tweeted his support of the team. Javier Hernandez's goal against Atletico ensured Real have won all of their eight matches without Bale . The Welshman took to Facebook on Wednesday to wish Real Madrid good luck against city rivals Atletico . Javier Hernandez's late goal ensured another victory for Ancelotti's men, taking their Bale-less record to eight wins, no draws and no losses. With the Welshman, Real have a 69 per cent win record, far from disastrous, recording 29 wins, four draws and nine losses. But the stats without him will surely give ammunition to those unhappy with their £86million man.\n",
            " The club will be a £ 1 - 0 in the World Cup . <unk> The club has been made by the Premier League squad . <unk> The former club 's side have been on the Premier League . <unk> The club 's side have been on Saturday .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misc Testing"
      ],
      "metadata": {
        "id": "7xtDW2OhgClw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('models/30_epoch_emb_size_256_half_dataset.pt')"
      ],
      "metadata": {
        "id": "CpCLOqcCgDVO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"\n",
        "Playing computer games such as Angry Birds teaches children important life skills including concentration, resilience and problem solving, an academic has said. Professor Angela Mcfarlane, an education expert who will become head of training body the College of Teachers next month, said many games were complex and required deep learning and lateral thinking to solve them. Prof Mcfarlane said she herself had become 'hooked' on the Lemmings computer game, as well as Angry Birds, and said such games could have a place in the classroom provided they were used under supervision. Professor Angela Mcfarlane says computer games like Angry Birds can teach children valuable life-skills . Expert: Prof Mcfarlane says the games can help children learn problem solving, resilience and concentration . She said: 'There are many computer games that require quite deep learning to master the games. 'Some of that learning applies beyond games to wider life, such as concentration, problem solving, and resilience - important life skills. 'Anyone who has tried to play complex video games will know they are difficult.' Speaking to The Times, Prof Mcfarlane said she had developed an obsession with both Angry Birds and a precursor, Lemmings, because they had made her think and get her strategy right. The education expert, who has advised the government on educational technology, and who is currently writing a book, Authentic Learning for the Digital Generation, said computer games could be used in the classroom to good effect provided it was done properly. Prof Mcfarlane said even pre-school children could benefit from games, as long as they were supervised and not just given a phone to play with to keep them quiet. Prof Mcfarlane said she herself had become 'hooked' on a computer game called Lemmings, pictured . She said some games could teach children fine motor control, or help with vocabulary or simple maths, and taught skills such as resilience that could be applicable to real life. Next month Prof Mcfarlane, who began her career as a secondary school teacher and head of department, will become chief executive and registrar of the College of Teachers, which offers professional training to teachers and support staff.\n",
        "\"\"\"\n",
        "\n",
        "summarize(model, article, decoding_method='greedy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "SrGcn5lvgN4h",
        "outputId": "4fcaa815-11c9-498a-e356-1b297790c1f3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The new book is part of her new book on Twitter . <unk> She says she will be happy with reading comprehension and leisure areas of thousands of her mother . <unk> The pair have been seen at her eyes , she says she will be able to be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTKyHEI5hWEc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}