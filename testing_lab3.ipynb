{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timoh/L90-Summarization/summarizer_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from run_abstractive_summarizer import main\n",
    "from models.abstractive_summarizer import AbstractiveSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "from models.abstractive_summarizer import AbstractiveSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"train_data\": \"data/train.json\",\n",
    "    \"validation_data\": \"data/validation.json\",\n",
    "    \"test_data\": \"data/test.json\",\n",
    "    'd_model': 512,\n",
    "    'nhead': 4,\n",
    "    'd_hid': 512,\n",
    "    'nlayers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'tokenizer': 'wordpiece',\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 10,\n",
    "    'grad_acc': 1,\n",
    "    'batch_size': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hparams['train_data'], 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(hparams['validation_data'], 'r') as f:\n",
    "    validation_data = json.load(f)\n",
    "    \n",
    "with open(hparams['test_data'], 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "\n",
    "train_articles = [article['article'] for article in train_data]\n",
    "train_summaries = [article['summary'] for article in train_data]\n",
    "\n",
    "val_articles = [article['article'] for article in validation_data]\n",
    "val_summaries = [article['summary'] for article in validation_data]\n",
    "\n",
    "test_articles = [article['article'] for article in test_data]\n",
    "test_summaries = [article['summary'] for article in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Washington (CNN) -- It is a case at the intersection of science and finance, an evolving 21st century dispute that comes down to a simple question: Should the government allow patents for human genes? The Supreme Court offered little other than confusion during oral arguments on Monday on nine patents held by a Utah biotech firm. Myriad Genetics isolated two related types of biological material, BRCA-1 and BRCA-2, linked to increased hereditary risk for breast and ovarian cancer. At issue is whether \"products of nature\" can be treated the same as \"human-made\" inventions, and held as the exclusive intellectual property of individuals and companies. A ruling is expected by late June. On one side, scientists and companies argue patents encourage medical innovation and investment that saves lives. On the other, patient rights groups and civil libertarians counter the patent holders are \"holding hostage\" the diagnostic care and access of information available to high-risk patients. How human genes become patented . Outside the court, several protesters held signs, such as \"Your corporate greed is killing my friends\" and \"My genes are not property.\" The justices asked tough questions, raising a number of colorful hypotheticals to explore the boundaries of patent law, including whether things like baseball bats, leaves from exotic Amazon River plants, and the human liver could get federal government protection. \"The patent law is filled with uneasy compromises, because on the one hand, we do want people to invent,\" said Justice Stephen Breyer. \"On the other hand, we\\'re very worried about them tying up some kind of whatever it is, particularly a thing that itself could be used for further advance.\" Justice Anthony Kennedy noted Myriad made a significant investment in time and money in its genetic \"discoveries\" and might be allowed to have two-decade control over the genes for research, diagnostics, and treatment. \"I just don\\'t think we can decide the case on the ground: oh, don\\'t worry about investment, it\\'ll come\" if there was no patent protection. But Justice Sonia Sotomayor used a \"chocolate chip\" cookie analogy to say merely isolating naturally derived products would normally not get you a patent, only for the particular process or use of the cookie. \"If I combust those in some new way, I can get a patent on that,\" she said. \"But I can\\'t imagine getting a patent simply on the basic items of salt, flour and eggs, simply because I\\'ve created a new use or a new product from those ingredients.\" The patent system was created more than two centuries ago with a dual purpose. One is to offer temporary financial incentives for those at the ground floor of innovative products like the combustible engine and the X-ray machine. The second is to ensure one company does not hold a lifetime monopoly that might discourage competition and consumer affordability. All patent submissions rely on a complex reading of applicable laws, distinguishing between abstract ideas and principles, and more tangible scientific discoveries and principles. Medical science had traditionally shunned patents. A famous example cited in numerous briefs in the current appeal involved Dr. Jonas Salk\\'s development and invention of the polio oral vaccine in 1952. When his life-saving treatment was announced, he said the people would \"own\" the vaccine, adding \"Could you patent the sun?\" Maybe not the sun, but in the past 31 years, 20 percent of the human genome has been protected under U.S. patents. All sides agree the science of isolating the building blocks of life is no easy task. Myriad has said it has spent several years and hundreds of millions of dollars in its research. Since Myriad owns the patent on breast cancer genes, it is the only company that can perform tests for potential abnormalities. An initial test catches most problems, but the company also offers a second, separate test, called BART, to detect the rest, a diagnostic that can cost several thousand dollars. \"We invest heavily in the research and development that is needed to discover and provide high-quality molecular diagnostic products that save and improve patients\\' lives,\\'\\' said Richard Marsh, executive vice president and general counsel at Myriad. \"Strong intellectual property and patent rights in the United States are critical to fulfilling our mission.\" The company says 1 million patients have benefited from its BRAC Analysis technology, and that about 250,000 such tests are performed yearly. And officials say the average out-of-pocket expense for the testing is only about $100, a figure disputed by the plaintiffs. All sides agree too, the BCRA testing has saved countless lives since it was first unveiled in 1996, giving at-risk women the option of having their breasts removed as a preventive measure. But some patients have complained charging so much for a second test is \"irresponsible,\" and a sign the bio-tech company is more interested in profits than patient care for those unable to afford the second analysis. It is a charge the company strongly denies. Among those challenging the Myriad patents are sisters Eileen Kelly and Kathleen Maxian. Kelly was diagnosed with breast cancer at age 40. The initial BCRA test proved negative, meaning her family members were not likely at risk. But Maxian later developed ovarian cancer. The second BART testing proved positive, meaning the siblings carry the cancer-causing mutation all along. \"It could have prevented my cancer, or at least caught it early,\" Maxian told CNN\\'s Senior Medical Correspondent Elizabeth Cohen. \"Now my cancer is so advanced, I have only a 20-percent chance of being alive five years from now. I\\'m so angry.\" Money was not an issue for them, but Kelly and Maxian, along with a coalition of physician groups and genetic counselors say Myriad has not made the BART tests widely available for patients without a strong family history of these kinds of cancers. \"Myriad did not invent the human genes at issue in this case,\" Christopher Hansen of the ACLU told the court at argument. He is representing the plaintiffs bring the initial lawsuit. \"Myriad deserves credit for having unlocked these secrets. Myriad does not deserve a patent for it.\" But several justices raised concerns. \"Isolating or extracting natural products has long been considered patentable,\" said Justice Ruth Bader Ginsburg, echoing what the company has argued - and won in lower courts. She offered examples like \"aspirin and whooping cough vaccine. How is this different from those starting as natural products?\" Justice Samuel Alito used the example of a newly discovered plant with \"tremendous medicinal purposes.\" \"It\\'s not just the case of taking the leaf off the tree and chewing it\" to treat breast cancer. \"Let\\'s say if you do that, you\\'d have to eat a whole forest to get the value of this. But it\\'s extracted and reduced to a concentrated form. That\\'s not eligible for a patent?\" Justices Elena Kagan and Antonin Scalia put a financial spin on the question. \"Why would a company incur massive investment if it cannot patent?\" an isolated gene, asked Scalia. When told by Hansen that scientific \"curiosity\" or Nobel Prize recognition might be enough, Scalia seemed unsatisfied. \"Well, that\\'s lovely,\" he said somewhat sarcastically. On the other side Myriad attorney Gregory Castanias said \"the parade of horribles\" raised by the plaintiffs had not happened. \"There\\'s no dispute in this case that there has been some alteration of the isolated DNA molecules.\" He said there has been an \"explosion in biotechnology and the successful, economically successful, technologically successful, and life-saving industry that is at the heart of these inventions.\" But Kennedy disputed Castanias\\' assertion this patent was deserved because of \"different economic values\" in cutting-edge genetic science-- and that patients did not have the \"human ingenuity\" of the BCRA test before Myriad developed it. \"Well, we could have said that with atomic energy, with electric, but so far the choice of the patent law was that we have a uniform rule for all industries,\" Kennedy said. \"That avoids giving special industries special subsidies, which is very important it seems to me.\" Chief Justice Roberts repeated the concerns of several colleagues, when saying the gene patents were much different from those for pharmaceuticals and other medical advances. \"There, you\\'re obviously combining things and getting something new. Here you\\'re just snipping [the genetic line], and you don\\'t have anything new, you have something that is a part of something that has existed previous to your intervention.\" The federal government is taking a somewhat middle ground: DNA itself is not patentable but so-called \"cDNA\" can be. Complementary DNA is artificially synthesized from the genetic template, and engineered to produce gene clones. Use of this protein-isolating procedure, known as \"tagging,\" is especially important in mapping and cataloguing the vast human genome. Solicitor General Donald Verrilli also suggested Congress could create exceptions in the genetic or bio-tech testing arena, over so-called \"use\" patents for specific procedures. The high court has addressed the broader issue before, and may use a 2012 decision to guide the current human gene case. In the so-called \"Mayo\" case, the justices unanimously said patents could not be issued on observations of a natural phenomenon -- in this case a doctor\\'s medical diagnosis of a patient\\'s reaction to a drug. \"If a law of nature is not patentable, than neither is a process reciting a law of nature, unless that process has additional features that provide practical assurance that the process is more than a drafting effort designed to monopolize the law of nature itself,\" wrote Breyer for the court in Mayo. The current case is Association for Molecular Pathology v. Myriad Genetics (12-398).'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(train_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Cheltenham have appointed Gary Johnson as manager until the end of the season. Johnson, who left his post in charge of Yeovil last month, has been tasked with saving the club, who are bottom of Sky Bet League Two, from relegation from the Football League. Gary Johnson has been appointed Cheltenham manager until the end of the season . The 59-year-old told the club's official website: 'I understand the position the club are in and I will be doing my best along with (assistant) Russell (Milton) to keep the club in the Football League.' Johnson was at Cheltenham's 2-2 draw with Portsmouth at Fratton Park on March 17 and the 2-1 home defeat by Exeter City four days later. 'I feel I know a lot about the squad, but it's also important to have that continuity with Russell and the lads so we'll be having a good chat regarding all of the players' abilities,' Johnson said. Cheltenham Town are bottom of Sky Bet League Two with seven games remaining . Milton, who has taken charge of nine matches since the departure of Paul Buckle in February, said: 'I am happy to be working alongside someone of Gary's knowledge and experience. 'I've been pleased with the way the players have responded since I have been in charge and I don't feel we are far away from getting things right. 'I feel performances have improved overall, with results hopefully now following.'\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(val_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 17:25:03,221 - INFO - Initializing tokenization vocabulary with method: wordpiece.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 17:25:03,487 - INFO - WordPiece tokenizer initialized with custom special tokens.\n",
      "/home/timoh/L90-Summarization/summarizer_venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = AbstractiveSummarizer(\n",
    "    hparams['d_model'], \n",
    "    hparams['nhead'], \n",
    "    hparams['d_hid'], \n",
    "    hparams['nlayers'], \n",
    "    hparams['dropout'],\n",
    "    hparams['tokenizer'],\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Playing computer games such as Angry Birds teaches children important life skills including concentration, resilience and problem solving, an academic has said. Professor Angela Mcfarlane, an education expert who will become head of training body the College of Teachers next month, said many games were complex and required deep learning and lateral thinking to solve them. Prof Mcfarlane said she herself had become 'hooked' on the Lemmings computer game, as well as Angry Birds, and said such games could have a place in the classroom provided they were used under supervision. Professor Angela Mcfarlane says computer games like Angry Birds can teach children valuable life-skills . Expert: Prof Mcfarlane says the games can help children learn problem solving, resilience and concentration . She said: 'There are many computer games that require quite deep learning to master the games. 'Some of that learning applies beyond games to wider life, such as concentration, problem solving, and resilience - important life skills. 'Anyone who has tried to play complex video games will know they are difficult.' Speaking to The Times, Prof Mcfarlane said she had developed an obsession with both Angry Birds and a precursor, Lemmings, because they had made her think and get her strategy right. The education expert, who has advised the government on educational technology, and who is currently writing a book, Authentic Learning for the Digital Generation, said computer games could be used in the classroom to good effect provided it was done properly. Prof Mcfarlane said even pre-school children could benefit from games, as long as they were supervised and not just given a phone to play with to keep them quiet. Prof Mcfarlane said she herself had become 'hooked' on a computer game called Lemmings, pictured . She said some games could teach children fine motor control, or help with vocabulary or simple maths, and taught skills such as resilience that could be applicable to real life. Next month Prof Mcfarlane, who began her career as a secondary school teacher and head of department, will become chief executive and registrar of the College of Teachers, which offers professional training to teachers and support staff.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Professor Angela Mcfarlane is an education expert and former teacher .\\nShe says complex computer games require concentration and resilience .\\nThe former government adviser says they also teach problem-solving .\\nProf Mcfarlane says she herself has been 'hooked' on Lemmings game .\\nThe academic is to become head of the College of Teachers next month .\\nShe is currently writing a book on education for the 'digital generation'\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and converting text to indices using WordPiece in batches.\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = model.preprocess(train_articles[:50], train_summaries[:50], hparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = []\n",
    "# for batch_idx, batch in tqdm.tqdm(enumerate(train_dataloader)):\n",
    "#     input_ids, attention_mask, labels, _ = batch\n",
    "#     for input_id in input_ids:\n",
    "#         lengths.append(len(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and converting text to indices using WordPiece in batches.\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = model.preprocess(val_articles[:50], val_summaries[:50], hparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and converting text to indices using WordPiece in batches.\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = model.preprocess(test_articles[:50], test_summaries[:50], hparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 17:28:59,405 - INFO - Beginning training.\n",
      "Training epoch 0: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s=tensor([  101,  2652,  3274,  2399,  2107,  2004,  4854,  5055, 12011,  2336,\n",
      "         2590,  2166,  4813,  2164,  6693,  1010, 24501, 18622, 10127,  1998,\n",
      "         3291, 13729,  1010,  2019,  3834,  2038,  2056,  1012,  2934, 10413,\n",
      "        11338, 23511,  1010,  2019,  2495,  6739,  2040,  2097,  2468,  2132,\n",
      "         1997,  2731,  2303,  1996,  2267,  1997,  5089,  2279,  3204,  1010,\n",
      "         2056,  2116,  2399,  2020,  3375,  1998,  3223,  2784,  4083,  1998,\n",
      "        11457,  3241,  2000,  9611,  2068,  1012, 11268, 11338, 23511,  2056,\n",
      "         2016,  2841,  2018,  2468,  1005, 13322,  1005,  2006,  1996,  3393,\n",
      "        25057,  2015,  3274,  2208,  1010,  2004,  2092,  2004,  4854,  5055,\n",
      "         1010,  1998,  2056,  2107,  2399,  2071,  2031,  1037,  2173,  1999,\n",
      "         1996,  9823,  3024,  2027,  2020,  2109,  2104, 10429,  1012,  2934,\n",
      "        10413, 11338, 23511,  2758,  3274,  2399,  2066,  4854,  5055,  2064,\n",
      "         6570,  2336,  7070,  2166,  1011,  4813,  1012,  6739,  1024, 11268,\n",
      "        11338, 23511,  2758,  1996,  2399,  2064,  2393,  2336,  4553,  3291,\n",
      "        13729,  1010, 24501, 18622, 10127,  1998,  6693,  1012,  2016,  2056,\n",
      "         1024,  1005,  2045,  2024,  2116,  3274,  2399,  2008,  5478,  3243,\n",
      "         2784,  4083,  2000,  3040,  1996,  2399,  1012,  1005,  2070,  1997,\n",
      "         2008,  4083, 12033,  3458,  2399,  2000,  7289,  2166,  1010,  2107,\n",
      "         2004,  6693,  1010,  3291, 13729,  1010,  1998, 24501, 18622, 10127,\n",
      "         1011,  2590,  2166,  4813,  1012,  1005,  3087,  2040,  2038,  2699,\n",
      "         2000,  2377,  3375,  2678,  2399,  2097,  2113,  2027,  2024,  3697,\n",
      "         1012,  1005,  4092,  2000,  1996,  2335,  1010, 11268, 11338, 23511,\n",
      "         2056,  2016,  2018,  2764,  2019, 17418,  2007,  2119,  4854,  5055,\n",
      "         1998,  1037, 14988,  1010,  3393, 25057,  2015,  1010,  2138,  2027,\n",
      "         2018,  2081,  2014,  2228,  1998,  2131,  2014,  5656,  2157,  1012,\n",
      "         1996,  2495,  6739,  1010,  2040,  2038,  9449,  1996,  2231,  2006,\n",
      "         4547,  2974,  1010,  1998,  2040,  2003,  2747,  3015,  1037,  2338,\n",
      "         1010, 14469,  4083,  2005,  1996,  3617,  4245,  1010,  2056,  3274,\n",
      "         2399,  2071,  2022,  2109,  1999,  1996,  9823,  2000,  2204,  3466,\n",
      "         3024,  2009,  2001,  2589,  7919,  1012, 11268, 11338, 23511,  2056,\n",
      "         2130,  3653,  1011,  2082,  2336,  2071,  5770,  2013,  2399,  1010,\n",
      "         2004,  2146,  2004,  2027,  2020, 13588,  1998,  2025,  2074,  2445,\n",
      "         1037,  3042,  2000,  2377,  2007,  2000,  2562,  2068,  4251,  1012,\n",
      "        11268, 11338, 23511,  2056,  2016,  2841,  2018,  2468,  1005, 13322,\n",
      "         1005,  2006,  1037,  3274,  2208,  2170,  3393, 25057,  2015,  1010,\n",
      "        15885,  1012,  2016,  2056,  2070,  2399,  2071,  6570,  2336,  2986,\n",
      "         5013,  2491,  1010,  2030,  2393,  2007, 16188,  2030,  3722,  8785,\n",
      "         2015,  1010,  1998,  4036,  4813,  2107,  2004, 24501, 18622, 10127,\n",
      "         2008,  2071,  2022, 12711,  2000,  2613,  2166,  1012,  2279,  3204,\n",
      "        11268, 11338, 23511,  1010,  2040,  2211,  2014,  2476,  2004,  1037,\n",
      "         3905,  2082,  3836,  1998,  2132,  1997,  2533,  1010,  2097,  2468,\n",
      "         2708,  3237,  1998, 24580,  1997,  1996,  2267,  1997,  5089,  1010,\n",
      "         2029,  4107,  2658,  2731,  2000,  5089,  1998,  2490,  3095,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0], device='cuda:0')\n",
      "t=tensor([  101,  2934, 10413, 11338, 23511,  2003,  2019,  2495,  6739,  1998,\n",
      "         2280,  3836,  1012,  2016,  2758,  3375,  3274,  2399,  5478,  6693,\n",
      "         1998, 24501, 18622, 10127,  1012,  1996,  2280,  2231, 11747,  2758,\n",
      "         2027,  2036,  6570,  3291,  1011, 13729,  1012, 11268, 11338, 23511,\n",
      "         2758,  2016,  2841,  2038,  2042,  1005, 13322,  1005,  2006,  3393,\n",
      "        25057,  2015,  2208,  1012,  1996,  3834,  2003,  2000,  2468,  2132,\n",
      "         1997,  1996,  2267,  1997,  5089,  2279,  3204,  1012,  2016,  2003,\n",
      "         2747,  3015,  1037,  2338,  2006,  2495,  2005,  1996,  1005,  3617,\n",
      "         4245,  1005,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n",
      "s=tensor([  101,  2011,  1012,  4074,  4829,  1012,  2405,  1024,  1012,  2410,\n",
      "         1024,  2260,  9765,  1010,  2260,  2255,  2262,  1012,  1064,  1012,\n",
      "         7172,  1024,  1012,  2410,  1024,  4700,  9765,  1010,  2260,  2255,\n",
      "         2262,  1012,  4126,  2369,  6963,  1024, 12587,  4679,  2097,  5247,\n",
      "         2178,  2702,  2086,  1999,  7173,  2044,  2002,  3040, 23356,  2098,\n",
      "         1037, 19690,  1998, 16034, 14513,  3388,  2013,  2010,  3526,  1012,\n",
      "         1037,  5850,  5797,  2097,  5247,  2178,  2702,  2086,  2369,  6963,\n",
      "         2005,  3040, 23356,  2075,  2178, 19690,  1998, 16034, 14513,  3388,\n",
      "         2013,  2010,  3827,  3526,  1012, 12587,  4679,  1010,  2861,  1010,\n",
      "         2001,  3529,  2698,  2086,  2005, 17731,  5850,  2000,  2010,  2188,\n",
      "         2103,  1997, 13299,  1011,  2006,  1011,  7990,  1010, 17052,  2664,\n",
      "         2002,  2211,  1037,  2047,  4735,  6960,  2043,  2002,  2108,  4015,\n",
      "         2000,  2019,  2330,  3827,  1012,  5819,  2006,  2154,  2713,  2013,\n",
      "         2167,  2712,  3409,  3827,  1999, 16628,  1010,  4679,  2109,  2010,\n",
      "         6107,  2004,  2019,  5976,  3105,  2158,  2006,  1996,  2648,  2004,\n",
      "         1037,  3104,  2000,  3113,  2039,  2007,  2010,  2214, 16222, 25377,\n",
      "        29146,  1012,  2002,  2059,  2719,  1037,  2047,  6080,  2029, 21877,\n",
      "        28090,  5850,  1999, 13299,  1011,  2006,  1011,  7990,  5819,  2002,\n",
      "         2366,  2041,  1996,  2717,  1997,  2010,  7683,  2058,  6036,  2661,\n",
      "         2185,  1012,  2002,  2130,  3266,  2000,  3477, 14534,  2629,  1010,\n",
      "         2199,  2013,  4319,  7149,  2046,  2010,  2219,  2924,  4070,  1012,\n",
      "         2610,  2211, 11538,  2044, 15103,  1037,  4125,  1999,  6355,  7362,\n",
      "         4126,  1999, 13299,  1998,  3603,  4679,  2001,  1996,  3614, 19000,\n",
      "         1012,  2027, 22333, 23029,  2012,  1996,  7173,  1998,  2211,  1037,\n",
      "         9867,  3169,  1998,  3603,  2008,  2002,  2001, 21317,  2047,  5850,\n",
      "         9144,  1012,  2198,  8109,  1010,  2756,  1010,  2001,  4679,  1521,\n",
      "         1055,  1520,  2157,  1011,  2192,  2158,  1521,  1999, 13299,  1011,\n",
      "         2006,  1011,  7990,  1998,  4758,  1996,  2154,  1011,  2000,  1011,\n",
      "         2154,  2770,  1997,  1996,  4319,  4126,  2096,  4679,  2815,  1999,\n",
      "         2369,  6963,  1999, 16628,  2073,  2002,  2001,  2056,  2000,  2022,\n",
      "         2311,  1037,  1520,  6625,  2166,  1521,  2013,  1996, 11372,  1012,\n",
      "         2610,  4727,  1999,  4679,  1999,  2238,  2197,  2095,  2044, 24681,\n",
      "         2062,  2084,  1016,  1012,  1016, 20850,  2015,  1997, 19690,  1998,\n",
      "        16034,  4276,  2105, 27708,  1010,  2199,  1998,  2478,  4684,  3042,\n",
      "         2636,  2000,  4957,  2659,  1011,  2504, 18092,  2015,  2000,  4679,\n",
      "         1998,  8109,  1012,  2012, 15064,  4410,  2457,  1999, 14161,  4904,\n",
      "         2669,  1010,  4679,  4914,  9714,  2000,  4425, 19690,  1998, 16034,\n",
      "         1012,  8109,  1997, 13299,  1010,  2001, 21278,  2005,  2698,  1998,\n",
      "         1037,  2431,  2086,  1998,  2178,  2698,  2273,  1998,  2048,  2308,\n",
      "         2020, 21278,  2005,  1037,  2561,  1997,  2603,  2086,  1012,  4458,\n",
      "         6251,  3648,  2745, 12731, 20845,  2409,  4679,  2651,  1024,  1520,\n",
      "         2017,  2018,  1037,  2535,  2029,  2001,  3811,  3278,  2030,  3675,\n",
      "         4179,  2877,  1012,  1520,  4415,  2017,  4146,  2023,  6960,  2096,\n",
      "         2218,  2012,  2019,  2330,  3827,  1012,  2023,  2003,  1037,  5667,\n",
      "        12943, 17643, 26477,  3444,  1012,  1521,  2280,  6883,  4679,  2001,\n",
      "         3322, 21278,  1999,  2268,  2044,  5825,  2019,  3169,  2000,  4425,\n",
      "         1996, 16743,  1997, 13299,  2007,  1016,  1012,  1016, 20850,  2015,\n",
      "         1997, 16034,  4276,  2039,  2000,  1069,  2620,  2620,  1010,  2199,\n",
      "         2029,  2002,  4149,  2013,  1037,  6220, 17264, 11033,  1012,  2047,\n",
      "         4735,  6960,  1024,  2096,  2006,  2154,  2713,  2013,  2167,  2712,\n",
      "         3409,  3827,  1006, 15885,  1007,  1999, 16628,  1010,  4679,  2109,\n",
      "         2010,   102], device='cuda:0')\n",
      "t=tensor([  101, 12587,  4679,  2001,  3529,  2698,  2086,  2005, 17731,  5850,\n",
      "         1999, 13299,  1011,  2006,  1011,  7990,  1012,  2002,  2211,  1037,\n",
      "         2047,  4735,  6960,  2043,  4015,  2000,  2019,  2330,  3827,  1012,\n",
      "         4679,  1005,  1055,  2047,  4126,  6080, 21877, 28090,  5850,  1999,\n",
      "        13299,  1011,  2006,  1011,  7990,  1012,  2002,  2001,  7331,  2000,\n",
      "         2178,  2702,  2086,  2369,  6963,  1012,  2698,  2273,  1998,  2048,\n",
      "         2308,  2020,  2036,  7979,  2058,  1996,  4126,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n",
      "s=tensor([  101,  2011,  1012,  7004,  2892,  3051,  1012,  2405,  1024,  1012,\n",
      "         5709,  1024,  4749,  9765,  1010,  2410,  2254,  2297,  1012,  1064,\n",
      "         1012,  7172,  1024,  1012,  2184,  1024,  2324,  9765,  1010,  2410,\n",
      "         2254,  2297,  1012, 28932,  2040,  2915,  1037,  2690,  1011,  4793,\n",
      "         2388,  1998,  2014,  2365,  1999,  1037, 17082,  9416,  2886,  2648,\n",
      "         1037, 27137,  2534,  2020,  2108, 14682,  2011,  2610,  2651,  1012,\n",
      "         3660, 19347,  1010,  2570,  1010,  1998,  2010,  5179,  1011,  2095,\n",
      "         1011,  2214,  2388,  5032,  2020,  3236,  1999,  1037, 16889,  1997,\n",
      "        10432,  2044,  2027,  2020, 22168,  1999,  1037,  2482,  2380,  2648,\n",
      "         1996, 16122, 27137,  3589,  1012, 18145,  2020,  2651,  2551,  2006,\n",
      "         1996,  3399,  2008,  2720, 19347,  1010,  2040,  3594,  1996,  8367,\n",
      "         1005,  1996,  5698,  1005,  2006,  2010,  9130,  3931,  1010,  2001,\n",
      "         1996,  3832,  4539,  1997,  1996,  5008,  1010,  1998,  2010,  2388,\n",
      "         2001,  3236,  1999,  1996,  2892, 10273,  1012,  3496,  1024,  2610,\n",
      "         3945,  1996,  2482,  2380,  2648,  1996, 16122, 27137,  1999, 14601,\n",
      "         3126, 14844,  1010,  5087,  1010,  2073,  2570,  1011,  2095,  1011,\n",
      "         2214,  3660, 19347,  1998,  2010,  5179,  1011,  2095,  1011,  2214,\n",
      "         2388,  5032,  2020,  2915,  1012,  4539,  1024, 18145,  2020,  2651,\n",
      "         2551,  2006,  1996,  3399,  2008,  2720, 19347,  2001,  1996,  3832,\n",
      "         4539,  1997,  1996,  5008,  1010,  1998,  2010,  2388,  2001,  3236,\n",
      "         1999,  1996,  2892, 10273,  1012, 18145,  2020,  2036,  3373,  2000,\n",
      "         2022, 11538,  4311,  1997,  1037,  7593,  2090,  2720, 19347,  1998,\n",
      "         2060,  2372,  1997,  1996,  2155,  1012,  5796, 19347,  1998,  2014,\n",
      "         2365,  2020,  4457,  3859,  2044,  2720, 19347,  3369,  2012,  1996,\n",
      "         2482,  2380,  2648,  1996, 27137,  2534,  2000,  4060,  2014,  2039,\n",
      "         2012,  2105,  1021,  1012,  2321,  9737,  2197,  2305,  1012,  5312,\n",
      "         2101,  1010,  1037,  2304,  2482,  2766,  2039,  2369,  1996,  2155,\n",
      "         1010, 10851,  2068,  1999,  1996,  2482,  2380,  2648,  1996, 14601,\n",
      "         3126, 14844,  3589,  1997,  4677, 16122, 27137,  1999,  5087,  1012,\n",
      "        15113,  1024,  1037,  2610, 11601,  2239,  2815,  2105,  1996,  3496,\n",
      "         2651,  2004,  3738,  2506,  2000,  8556,  1996,  5008,  1012,  2048,\n",
      "         2273,  2059,  6589,  2041,  1997,  1996,  2482,  1998,  5045,  1037,\n",
      "         2193,  1997,  7171,  2012,  1996,  2720, 19347,  1005,  1055, 20075,\n",
      "         1012,  2028,  7960,  2718,  5796, 19347,  1999,  1996,  8999,  1010,\n",
      "         2096,  2014,  2365,  2001,  2915,  1999,  1996,  7223,  1999,  1996,\n",
      "         2886,  1012,  2720, 19347,  3266,  2000,  2131,  2041,  1997,  1996,\n",
      "         2482,  2380,  1998,  4704,  2010,  2482,  1999,  1037,  3518,  2395,\n",
      "         2077,  2002,  1998,  2010,  5229,  2388, 14648,  2000,  2902,  1012,\n",
      "         1996,  7205,  2902,  1010,  2167,  5087,  2236,  1010,  2003,  2055,\n",
      "         1037,  2431,  1011,  3178,  3328,  2185,  1012,  7435,  2059,  2170,\n",
      "         2610,  2000,  8556,  1996,  5008,  1012,  4445,  4544,  2001,  3373,\n",
      "         2000,  2022,  2166,  1011,  8701,  1012,  1996,  5043,  4158,  1999,\n",
      "         1996, 24842,  3723,  5087,  2212,  1997, 14601,  3126, 14844,  2073,\n",
      "         6801,  4035,  2093,  4507,  2694,  2265,  2111,  2066,  2149,  2001,\n",
      "         2241,  1011,  1998,  2029,  2001, 11321,  6090,  7228,  2005,  4760,\n",
      "         1037, 12991, 27086,  3193,  1997,  1996, 10575,  1012,  2074,  2781,\n",
      "         2077,  1996,  5008,  2720,  1012, 19347,  6866,  1037,  4471,  2006,\n",
      "         2010,  9130,  3931,  3038,  1024,  1005, 17079,  2033,  1999,  1996,\n",
      "         1012,  2067,  1010,  1045,  1005,  2222,  8479, 24471,  8560,  1012,\n",
      "         1005,  2178,  6866,  2006,  2254,  1019,  2056,  1024,  1005,  1050,\n",
      "         2023,  2208,  2045,  1005,  1055,  2069,  2028,  2123,  1005,  1998,\n",
      "         2006,   102], device='cuda:0')\n",
      "t=tensor([  101, 28932,  2441,  2543,  2004,  3660, 19347,  3856,  2010,  2388,\n",
      "         5032,  2039,  2044, 27137,  1012,  5032,  1010,  5179,  1010,  2001,\n",
      "         2915,  1999,  1996,  8999,  2096,  2365,  3660,  1010,  2570,  1010,\n",
      "         2001,  4930,  2006,  1996,  7223,  1012,  3940,  2020,  8534,  2046,\n",
      "         2482,  2380,  2077,  2048,  2273,  5045,  7171,  2012,  2068,  1012,\n",
      "         2155,  3266,  2000,  4019,  1998,  2081,  2037,  2126,  2000,  2902,\n",
      "         2006,  3329,  1012, 18145,  2551,  2006,  3399,  2008,  2720, 19347,\n",
      "         2001,  1996,  3832,  4539,  1012,  5032,  2071,  2031,  2042,  3236,\n",
      "         1999,  1996,  2892, 10273,  1012,  2610, 11538,  4311,  1997,  1037,\n",
      "         2155,  7593,  5994,  2720, 19347,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n",
      "s=tensor([  101,  2011,  1012,  7939,  2072, 11332,  7103,  1012,  2405,  1024,\n",
      "         1012,  5757,  1024,  5709,  9765,  1010,  2403,  2238,  2286,  1012,\n",
      "         1064,  1012,  7172,  1024,  1012,  5757,  1024,  2539,  9765,  1010,\n",
      "         2403,  2238,  2286,  1012,  1996,  4658,  4355, 11028,  2005, 29118,\n",
      "         2047, 10756,  2003,  2182,  1012,  3008,  2071,  2574,  2022,  2583,\n",
      "         2000,  2202,  2037,  1012,  6927, 12910,  2041,  2006, 17260, 21172,\n",
      "         9109,  2007,  2068,  1011,  2004,  1037,  5245, 24925,  2099,  2038,\n",
      "         2042,  1012,  8826,  2008, 13585,  1996,  2048,  1012,  1996, 11333,\n",
      "        17413, 24528, 16790,  3774,  1997,  2019, 17876, 17260,  6277,  1010,\n",
      "         1012,  2124,  2004,  1037,  2146,  6277,  1010,  2029,  2038,  1996,\n",
      "         2835,  2930,  1997,  1037,  2775,  1005,  1055, 11829,  6292,  4987,\n",
      "         2000,  1012,  1996,  2392,  1997,  2009,  1012,  2336,  2090,  1996,\n",
      "         5535,  1997,  3157,  2706,  1998,  2176,  1011,  1998,  1011,  1037,\n",
      "         1011,  2431,  1012,  2086,  2214,  2064, 18579,  4133,  1999,  1996,\n",
      "        11829,  6292,  2096,  2037,  3008,  2064,  6154,  2006,  1996,  1012,\n",
      "         2067,  1998, 17678,  2884,  2009,  2247,  1012,  1996, 13576,  4031,\n",
      "         9188,  1996,  2146, 15271, 13181, 10820,  2038,  2036,  1012,  2042,\n",
      "         9412,  2007,  1037, 13428,  2012,  1996,  2067,  1998,  5047,  8237,\n",
      "         2015,  2005,  9602,  2369,  1996,  1012,  2835,  2930,  1012, 17186,\n",
      "         2091,  2005,  2678,  1012, 29118,  3008,  2071,  2574,  2022,  2583,\n",
      "         2000,  2202,  2037,  6927, 12910,  2041,  2006, 17260, 21172,  9109,\n",
      "         2007,  2068,  1012,  2009,  2097,  2036,  2812,  3008,  2097,  2022,\n",
      "         2583,  2000,  2202,  2037,  1012,  2336,  2041,  2006,  2172,  2936,\n",
      "         1005,  7365,  1005,  2027,  2064,  2007,  1037,  3671, 27244,  2121,\n",
      "         1012,  1996,  4031,  2038,  2042,  2580,  2011,  2640,  2996,  2996,\n",
      "         2848,  1012,  3158, 15544,  3388,  1999, 14003,  1010,  5706,  1010,\n",
      "         2005, 11829,  6292,  4435,  8804,  2100,  1012, 12235,  2015,  2369,\n",
      "         1996,  2146, 15271, 13181, 10820,  3246,  2009,  2097,  2022,  2019,\n",
      "         1012, 25262,  5379,  4522,  2000,  8932,  1999,  3655,  1998,  2008,\n",
      "         3008,  1012,  2097,  5454,  2009,  2058,  1037,  2482,  2030,  3902,\n",
      "         1012,  2105,  2809, 19599,  1997,  1996,  4031,  2031,  2042,  8826,\n",
      "         1012,  1998,  2009,  2003,  2747,  2108,  7718,  1998, 16330,  2005,\n",
      "         2740,  1998,  3808,  5761,  1012,  2848,  3158, 15544,  3388,  1010,\n",
      "         4413,  1010,  3954,  1997,  2996,  2848,  3158, 15544,  3388,  1010,\n",
      "         2056,  1024,  1005,  2197,  2095,  2057,  2020,  5411,  2011,  8804,\n",
      "         2100,  1998,  2027,  2018,  2019,  2801,  2000,  2191,  1037,  1012,\n",
      "        27244,  2121,  2017,  2071,  3233,  2006,  1012,  1005,  2057,  2777,\n",
      "         2007,  3008,  1051,  2831,  2055,  2009,  1998,  2234,  2039,  2007,\n",
      "         1012,  1996,  2801,  1997,  1037, 27244,  2121,  4117,  2007,  1037,\n",
      "         2146,  2604,  1012,  1996, 11333, 17413, 24528, 16790,  3774,  1997,\n",
      "         1037,  2146,  6277,  2029,  2038,  1996,  2835,  2930,  1997,  1037,\n",
      "         2775,  1005,  1055, 11829,  6292,  4987,  1012,  2336,  2090,  1996,\n",
      "         5535,  1997,  3157,  2706,  1998,  2176,  1011,  1998,  1011,  1037,\n",
      "         1011,  2431,  2086,  2214,  2064, 18579,  4133,  1999,  1996, 11829,\n",
      "         6292,  1012,  1005,  2007,  2074,  1037, 27244,  2121,  2894,  2017,\n",
      "         2064,  3604,  2672,  2431,  1037,  1012,  3542,  1998,  2023,  4107,\n",
      "         1037,  5576,  2000,  2175,  2582,  2302,  2383,  2000,  2224,  1037,\n",
      "         2482,  2030,  1012,  2178,  2433,  1997,  3665,  1012,  1005,  2009,\n",
      "         1005,  1055,  2036, 25262,  5379,  2029,  2003,  2028,  1997,  1996,\n",
      "         1012,  2364,  2685,  2369,  2009,  1012,  1005,  2000,  2191,  1996,\n",
      "         8773,  2057,  2109,  2112,  1997,  2019,  4493,  1012, 27244,  2121,\n",
      "         1998,   102], device='cuda:0')\n",
      "t=tensor([  101, 11829,  6292,  4435,  8804,  2100,  5411,  2640,  2996,  2996,\n",
      "         2848,  3158, 15544,  3388,  1012,  2356,  2068,  2000,  2191,  1037,\n",
      "        27244,  2121,  2017,  2071,  3233,  2006,  1012,  2146, 15271, 13181,\n",
      "        10820,  2747,  2108,  7718,  2005,  2740,  1998,  3808,  5761,  1012,\n",
      "         2000,  3465,  2012,  2560, 28182,  8889,  1998,  4473,  2017,  2000,\n",
      "         2175,  2582,  2302,  2478,  2482,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n",
      "s=tensor([  101,  2011,  1012,  4775, 25663,  1012,  2197,  7172,  2012,  1021,\n",
      "         1024,  2656,  7610,  2006,  5351,  2255,  2249,  1012,  2027,  4061,\n",
      "         1010,  1998,  4265,  6659,  6441,  1010,  2006, 11686,  2015,  2408,\n",
      "         1996,  2088,  1012,  2085,  3998,  5303,  2030, 14868,  1011, 12603,\n",
      "         7315,  2098,  3548,  2097,  2031,  1037,  2210,  3538,  1997,  3725,\n",
      "         2000,  2655,  2037,  2219,  1012,  1037, 13280, 21853,  2581, 19912,\n",
      "         3258,  8244,  1005,  2352,  2003,  2275,  2000,  2022,  2328,  2006,\n",
      "         1037,  4583,  1011,  7456,  2609,  1999,  1996, 12916, 10833,  1012,\n",
      "        13280,  1024,  1037,  2047,  7822,  2003,  2108,  2328,  2005,  5229,\n",
      "         8244,  1998,  2097,  2421,  1037,  3929,  1011, 21121,  3938,  1011,\n",
      "         2793,  8329,  1998,  2729,  4322,  1006,  1016,  1007,  1010,  3263,\n",
      "        27563,  2015,  1006,  1021,  1007,  1010,  1037, 12257,  2803,  1006,\n",
      "         1017,  1007,  1998,  1037, 20325,  8493,  1006,  2340,  1007,  1996,\n",
      "         2352,  2097,  2421,  1037,  3929,  1011, 21121,  3938,  1011,  2793,\n",
      "         8329,  1998,  2729,  4322,  1010,  3263, 27563,  2015,  1010, 12257,\n",
      "         2803,  2007,  5742,  4770,  1010,  2591,  2252,  1010,  6597,  1010,\n",
      "        11051,  7365,  1998,  1037, 20325,  8493,  1012,  1999,  1037,  2034,\n",
      "         1997,  2049,  2785,  1999,  1996,  2866,  1010,  2049,  2313,  2097,\n",
      "         2471,  4498,  2022,  2081,  2039,  1997,  2280,  3629,  2040,  2031,\n",
      "         4265, 23512,  3558,  2030,  8317,  6441,  2006,  1996, 11686,  1012,\n",
      "         1037,  2312,  2193,  1997, 26714,  1998,  2308,  2097,  2031,  2042,\n",
      "         6649,  1011,  5303,  1999,  1996,  3522,  9755,  1999,  5712,  1998,\n",
      "         7041,  1012,  1996,  2622,  1010, 13830,  2039,  2011,  1996,  8003,\n",
      "         1005,  1055,  7822,  5952,  1010,  2052,  2022,  2328,  2006,  1037,\n",
      "         2280, 14090,  9316,  2485,  2000,  1996,  8429,  1997,  2047,  8653,\n",
      "         1010, 16205,  1010,  1998,  1996,  2352,  1997,  1041,  9818, 15689,\n",
      "         3334,  1010,  2221,  9296,  1012,  1996,  9360,  2097,  6148,  4041,\n",
      "         6656,  1999,  2285,  1998,  3246,  2000,  2330,  1996,  2034,  5014,\n",
      "         1998,  2729,  4322,  2397,  2279,  2095,  1012,  2750,  5936,  2013,\n",
      "         2334,  3901,  2008,  1996,  2047,  2458,  2097,  1038,  7138,  1996,\n",
      "        10833,  1010,  1996,  5952,  2685,  2041,  2008,  2009,  2003,  2006,\n",
      "         1037,  2829,  1011,  2492,  2609,  2008,  7431,  1037,  4713,  1012,\n",
      "         1996,  8244,  1005,  2352,  2097,  2036,  3443,  2039,  2000,  3156,\n",
      "         5841,  1999,  2019,  2181,  2718,  2524,  2011, 12163,  1998, 10216,\n",
      "         2769,  2046,  1996,  2334,  4610,  1012,  1996,  3488,  2020,  3390,\n",
      "         2011,  9993, 14265,  3779,  2022,  8167,  2854,  1010,  1996,  6587,\n",
      "         5268,  2412,  2000,  2022,  3018,  1996,  3848,  2892,  1011,  1996,\n",
      "         3284,  2510, 11446,  2005, 11748,  8162,  1012,  4888,  1024,  9993,\n",
      "        14265,  3779,  2022,  8167,  2854, 18315,  3271,  2000,  4895,  3726,\n",
      "         4014,  1996,  2047,  7822,  3041,  2023,  2733,  1998,  2056,  2009,\n",
      "         2052,  3749,  2242,  2053,  3460,  2071,  1012,  9993, 18133,  2140,\n",
      "         2022,  8167,  2854,  1010,  2085,  3590,  1010,  1997,  1996,  3083,\n",
      "         1012,  4123,  1010,  4615,  1997,  3575,  1521,  1055,  2548,  3483,\n",
      "         1010,  2001, 15546,  2005,  7494,  1012,  2372,  1997,  2010,  3131,\n",
      "         2013, 15283,  2229,  2006,  2089,  1015,  1998,  2238,  2340,  1010,\n",
      "         2432,  1010,  1999,  2632,  1011,  1012, 28599,  2232,  1010,  5712,\n",
      "         1012,  2002,  4265,  1012,  3809,  2132,  6441,  2044,  1037,  7596,\n",
      "        15801, 17038,  2718,  1037,  4316,  2416,  1012,  5282,  2013,  2010,\n",
      "         1010,  4786, 14021,  2527,  2361, 11877,  6441,  2000,  2010,  2227,\n",
      "         1998,  4167,  1012,  2664,  2002,  3266,  2000,  8011,  2195, 19033,\n",
      "         2000,  3808,  1012,  2002,  2056,  1024,  1005,  2005,  4364,  2000,\n",
      "         2022,   102], device='cuda:0')\n",
      "t=tensor([  101,  4322,  2003,  2034,  1997,  2049,  2785,  1999,  1996,  2866,\n",
      "         1012,  5952,  3791, 27708,  1010,  2199,  2000,  2131,  2622,  2039,\n",
      "         1998,  2770,  1012,  2582,  4751,  2024,  2800,  2012,  7479,  1012,\n",
      "         8244, 13465, 16416,  2102,  1012,  8917,  1012,  2866,  1012,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n",
      "s=tensor([  101,  2414,  1006, 13229,  1007,  1011,  1011,  4238,  2038,  2042,\n",
      "         4755,  2041, 19813,  5852,  1997, 10815,  5214,  1997, 12771,  1037,\n",
      "         4517, 18093,  1010,  2866,  3097,  3187,  2520, 14575,  2056,  9317,\n",
      "         1010,  1999, 24528, 15338,  3258,  1997,  1037,  1057,  1012,  1050,\n",
      "         1012,  5813,  1012,  2009,  2038,  2036,  2056,  2009,  4122,  2000,\n",
      "         4372, 13149, 14247,  2000,  1000,  3798,  2521,  3618,  2084,  2003,\n",
      "         2734,  2005,  9379,  4517,  2943,  1010,  1000, 14575,  2056,  1012,\n",
      "         4238,  1005,  1055,  2458,  1997,  7421,  1998,  4517,  4762,  2974,\n",
      "         2038,  2419,  2000,  1057,  1012,  1050,  1012, 17147,  1998, 13519,\n",
      "         2013,  1996,  2142,  2163,  2008,  1996, 23106,  6939,  2003,  2667,\n",
      "         2000,  4503,  4517,  4255,  1012,  4238,  2758,  2009,  2038,  1037,\n",
      "         2157,  2000,  9379,  4517,  2974,  1010,  2021,  1996,  2248,  9593,\n",
      "         2943,  4034,  1006, 24264,  5243,  1007,  2758,  2009,  3685, 20410,\n",
      "         3251, 13503,  1005,  1055,  4517,  2565,  3464,  4498,  9379,  1012,\n",
      "         4238,  2038,  2025,  2664,  5838,  2000, 14575,  1005,  1055,  4447,\n",
      "         1012,  2010,  7928,  1999,  1996,  2160,  1997,  7674,  2272,  1037,\n",
      "         2154,  2044,  7726,  2739,  6736,  2988,  2008,  1996,  2406,  1005,\n",
      "         1055,  2510,  2018,  5147,  3231,  1011,  5045,  2403, 10815,  2076,\n",
      "         2510, 28308,  1010,  2004,  2112,  1997,  1037,  2733,  1997,  2162,\n",
      "         2399,  1012, 13503,  2036, 11521,  2195, 19630,  7421,  9033, 10483,\n",
      "         6928,  1010,  1998,  9317,  7645,  2054,  2001,  2649,  2011,  1996,\n",
      "         4100,  1011,  2880,  2521,  2015,  2739,  4034,  2004,  1037,  2047,\n",
      "         7726,  1011,  2081,  2146,  1011,  2846,  7217,  2291,  5214,  1997,\n",
      "         8822,  2659,  1011,  7998, 14549,  1012,  4092,  1999,  2414,  1010,\n",
      "        14575,  2056,  1024,  1000,  4238,  2038,  2036,  2042,  4755,  2041,\n",
      "        19813, 19630,  7421,  5852,  1998,  7596, 18989,  1010,  2164,  5604,\n",
      "        10815,  5214,  1997, 12771,  1037,  4517, 18093,  1999, 24528, 15338,\n",
      "         3258,  1997,  1057,  1012,  1050,  1012,  5813,  4612,  1010,  1998,\n",
      "         2009,  2038,  2623,  2008,  2009, 18754,  2000,  6420,  2049,  3977,\n",
      "         2000,  3965,  2322,  1003, 25202, 14247,  1012,  1000,  1996,  2866,\n",
      "         3097,  3187,  5763,  2000,  1000,  5441,  1998,  3613,  2000,  3623,\n",
      "         3778,  2006,  4238,  2000, 13676,  2019,  3820,  2006,  2037,  4517,\n",
      "         4746,  1010,  2311,  2006,  1996, 16003,  1997, 17147,  1010,  1000,\n",
      "         2623,  3041,  2023,  3204,  1012, 25202, 14247,  2064,  2022,  2109,\n",
      "         2005,  6831,  4517,  5682,  1010,  2021,  2036,  2000,  2191,  9593,\n",
      "         4255,  1012,  1996, 24264,  5243,  2992,  5936,  1999,  2337,  2008,\n",
      "         4238,  2001,  2025, 11973,  2007,  1996,  4034,  2006,  4447,  2008,\n",
      "         2009,  2001,  4975,  1037,  4517, 18093,  2005,  2049, 10815,  1012,\n",
      "         1996, 24264,  5243,  3189,  6936,  2825,  4517,  3450,  5079,  2000,\n",
      "         4238,  1005,  1055,  2510,  1000,  2164,  3450,  3141,  2000,  1996,\n",
      "         2458,  1997,  1037,  4517, 18093,  2005,  1037,  7421,  1012,  1000,\n",
      "         1996,  2142,  2163,  9770,  2047, 17147,  2006,  4238,  2197,  2733,\n",
      "         1010, 14126,  1996,  2406,  1005,  1055,  2120,  8582,  1010,  4238,\n",
      "         2250,  1010,  2060,  3316,  1010,  1998,  2248,  2449, 12706,  5496,\n",
      "         1997, 25049,  2135,  2311,  2039, 13503,  1005,  1055,  2510,  1012,\n",
      "         4238, 10033,  2216, 17147,  1999,  1037,  3661,  2000,  1996,  2142,\n",
      "         3741,  1010,  3038,  1996,  2248,  2303,  2018,  1037,  5368,  2000,\n",
      "         2298,  2041,  2005,  1000,  3741,  2040,  2024,  6414, 23855,  2075,\n",
      "         2000, 18759,  2037, 11476,  1998,  6151, 19825,  3468,  2916,  2104,\n",
      "         2248,  2375,  1012,  1000,  1996,  2142,  3741,  2979,  1037,  2959,\n",
      "         2461,  1997, 17147,  2114,  4238,  1999,  2238,  2230,  1010,  6461,\n",
      "         2012,   102], device='cuda:0')\n",
      "t=tensor([  101,  3097,  3187,  2758,  4238,  2036,  4122,  2000,  4372, 13149,\n",
      "        14247,  3458,  9379,  3798,  1012,  2010, 12629,  2272,  1037,  2154,\n",
      "         2044,  4238,  3231,  1011,  5045,  2403, 10815,  1012,  5852,  2024,\n",
      "         1999, 24528, 15338,  3258,  1997,  1037,  1057,  1012,  1050,  1012,\n",
      "         5813,  1010, 14575,  2758,  1012,  1996,  2142,  2163,  9770,  2047,\n",
      "        17147,  2114,  4238,  2197,  2733,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s=tensor([  101,  2111,  4147,  5499,  1011,  2806,  5929,  1998,  2312, 10154,\n",
      "         2015,  2024,  7917,  2013,  5559,  2270,  7793,  2076,  1996, 25904,\n",
      "         2998,  2399,  1999,  7855,  2859,  1010,  1037,  4750,  2283,  1011,\n",
      "         2448,  3780,  4311,  1012,  1996,  7221,  2253,  2046,  3466,  6928,\n",
      "         1999,  1996,  2103,  1997, 13173, 27871,  1010,  1999,  1996,  2406,\n",
      "         1005,  1055,  2717,  3512,  7855, 25904,  2555,  1012,  2009, 12033,\n",
      "         2000,  5467,  2040,  4929, 15562,  2015,  1010,  2312, 10154,  2015,\n",
      "         1010,  2004,  2092,  2004,  2093,  4127,  1997,  5499, 14464,  1011,\n",
      "         1011,  2164,  2216,  2007,  1996,  2732,  1998, 13152,  6454,  1011,\n",
      "         1011,  2429,  2000,  1996, 13173, 27871,  3679,  1010,  1037,  3780,\n",
      "         6989,  2000,  1996,  2822,  4750,  2283,  1012,  9877,  1997,  2270,\n",
      "         3902,  3703,  1999,  1996,  2103,  2097,  2022, 15371,  2011,  3036,\n",
      "         5073,  2000,  6204, 14148,  1012,  1996,  3902,  7221,  2003,  2108,\n",
      "         2000, 12926,  2004,  2028,  1997,  1037,  2193,  1997,  1037,  3036,\n",
      "         5761,  2004,  1996,  2103,  6184,  1996,  6122, 25904,  2998,  2399,\n",
      "         1010,  1037,  2312,  2724,  1999,  1996,  8392,  2555,  1010,  1996,\n",
      "         3259,  2056,  1010,  8951,  1996,  2724,  1005,  1055, 10863,  2837,\n",
      "         1012,  1996,  7221,  2097,  2022, 16348,  2083,  2257,  2322,  1010,\n",
      "         2043,  1996,  2399,  2272,  2000,  2019,  2203,  1012,  4584,  2056,\n",
      "         2216,  2040, 23640,  1996,  7221,  2097,  2022,  2988,  2000,  1996,\n",
      "         2610,  1010,  1996,  3259,  2988,  1012,  1057,  2100,  5603,  3126,\n",
      "         2916,  2967,  2031,  6367,  1996,  5468,  1010,  4214,  2009,  1037,\n",
      "         1000,  2665,  1011,  2422,  1000,  2005, 14398,  1012,  1000,  4584,\n",
      "         1999, 13173, 27871,  2103,  2024,  2203,  5668,  2075,  2019, 10132,\n",
      "        16939,  1998,  5860, 20026, 28230,  3343,  6461,  2012,  6623,  1057,\n",
      "         2100,  5603,  3126,  2111,  1010,  1000,  1057,  2100,  5603,  3126,\n",
      "         2137,  2523,  2343,  1010,  4862,  2213,  7367, 22123,  7245,  2056,\n",
      "         1999,  1037,  4861,  1012,  1000,  1996,  5142,  2003,  4584,  2514,\n",
      "         2027,  2064, 13109,  4887,  3372,  1996,  4277,  1997,  2859,  1999,\n",
      "         2344,  2000, 10408,  8418,  9743,  4691,  1005,  1055,  2028,  2095,\n",
      "         3424,  1011,  7404,  3049,  2008,  3544,  2062,  1998,  2062,  2000,\n",
      "         2022,  1037, 17264, 28644,  2114,  1996,  1057,  2100,  5603,  3126,\n",
      "         7243,  1005,  3412,  9029,  1998,  6078,  1012,  1000,  2054,  2023,\n",
      "         3902,  7221,  4136,  2149,  2003,  2008,  1057,  2100,  5603,  9236,\n",
      "         1999,  2859,  2031,  8491,  2916,  2084,  2060,  4480,  1010,  2130,\n",
      "         1999,  2037, 10759,  1010,  1998,  2008,  2009,  2665,  1011,  4597,\n",
      "         9147,  2114,  1057,  2100,  5603,  9236,  2011,  6623,  7658,  2822,\n",
      "         1012,  1000,  9877,  1997,  2270,  3902,  3703,  1999,  1996,  2103,\n",
      "         2097,  2022, 15371,  2011,  3036,  5073,  2000,  6204, 14148,  1012,\n",
      "         1996,  7221,  2006,  3902,  8195,  4147, 10154,  2015,  1998,  5499,\n",
      "         1011,  2806,  5929,  2515,  2025,  4919,  2309,  2041,  1057,  2100,\n",
      "         5603,  9236,  1010,  2021,  2049,  2653,  4415,  2685,  2000,  2068,\n",
      "         2004,  4022,  7889,  1997,  1996,  9259,  1012,  2009,  3310,  2055,\n",
      "         1037,  3204,  2044,  1996,  2822,  4614,  1999,  1996,  2555,  7917,\n",
      "         2493,  1998,  2942,  8858,  2013,  3435,  2075,  2076, 14115,  7847,\n",
      "         1010,  1996,  5152,  4151,  3204,  1012,  1999,  3522,  2086,  1010,\n",
      "         1996, 25904,  2555,  2038,  2464,  2019,  2039, 26348,  1999,  4808,\n",
      "         2090,  1057,  2100,  5603,  9236,  1998,  7658,  2822,  2058,  3171,\n",
      "         1998,  2060,  3314,  1012,  2197,  2733,  1010,  3053,  2531,  2111,\n",
      "         1011,  1011,  2087,  1997,  2068,  1000, 15554,  1010,  1000,  2429,\n",
      "         2000,  2110,  2865,  1011,  1011,  2020,  2730,  1999,  2019,  2886,\n",
      "         1999,   102], device='cuda:0')\n",
      "t=tensor([  101,  3902,  7221,  6367,  2004,  1000,  2665,  1011,  2422,  1000,\n",
      "         2005, 14398,  2114,  1057,  2100,  5603,  9236,  1999, 25904,  1012,\n",
      "         5761,  9770,  2076, 25904,  2998,  2399,  2029,  4515,  2006,  2257,\n",
      "         2322,  1012,  7221,  2950,  4147,  5499,  1011,  2806,  5929,  2006,\n",
      "         2270,  7793,  1012,  2273,  2007,  2312, 10154,  2015,  2024,  2036,\n",
      "         7917,  2013,  2635,  1996,  3902,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n",
      "s=tensor([  101,  2011,  1012,  6330, 23776,  1012,  2405,  1024,  1012,  2322,\n",
      "         1024,  4583,  9765,  1010,  1023,  2285,  2286,  1012,  1064,  1012,\n",
      "         7172,  1024,  1012,  5840,  1024,  2753,  9765,  1010,  2184,  2285,\n",
      "         2286,  1012,  1037,  2534, 14194,  5740, 16505,  4319,  2109,  1999,\n",
      "        23610,  2594, 13549,  1999,  1996,  9733,  2003,  3652,  1999,  6217,\n",
      "         1999,  1996,  1057,  1012,  1055,  1012,  1010,  6950,  2031,  2179,\n",
      "         1012, 27211, 16921,  7096,  2854, 22799, 11233,  1010,  2030,  1040,\n",
      "        20492,  1010,  2003,  1996,  3161, 21774,  1999,  1037, 17560,  6692,\n",
      "        15782,  1010,  2029,  2003,  1037,  3269,  1011,  2241,  8150,  2008,\n",
      "         2064,  2036,  2022, 20482,  2000,  2507,  5198,  1037,  2534, 14194,\n",
      "        28230,  1005,  4990,  1005,  2062,  6034,  2084,  2008,  1997,  1048,\n",
      "        16150,  1010, 17710, 15464,  3170,  2030,  3894, 23827,  1012,  4205,\n",
      "         5222, 17406,  1010,  3910,  1997,  1996,  3795,  5850,  5002,  1010,\n",
      "         4447,  2009,  2038,  1005,  1037,  3469, 10817,  1997,  2047,  5198,\n",
      "         1005,  4102,  2000,  2060,  3928,  5850,  1012,  1037, 17560,  6692,\n",
      "        15782,  1024,  1037, 23610,  1999,  1996, 28155, 15143,  2555, 26077,\n",
      "         2015,  3727,  2005,  2037, 18224, 19620,  5144,  2004,  2109,  1999,\n",
      "         1037, 17560,  6692, 15782,  1010, 10378,  1010,  2268,  1012,  2023,\n",
      "         6083,  1005,  2049,  6217,  2089,  3623,  1010,  1005,  5222, 17406,\n",
      "         2758,  1999,  1996,  6745,  3277,  1997,  1996,  3485,  1997, 18224,\n",
      "        21890, 17830, 19824,  1010,  2429,  2000,  1996, 26149,  2695,  1012,\n",
      "         1996,  5002,  1005,  1055,  9556,  2024,  3569,  2011,  3463,  1997,\n",
      "         2178,  2817,  1010,  1996,  2120,  5002,  2006,  4319,  2224,  1998,\n",
      "         2740,  1010,  4146,  2011,  1996,  2231,  1010,  2029,  2179,  1996,\n",
      "         2193,  1997,  2111,  2040,  2031,  2109,  1040, 20492,  2012,  2070,\n",
      "         2391,  2038,  2915,  2039,  2013,  2019,  4358,  6273,  2620,  1010,\n",
      "         2199,  1999,  2294,  2000,  1015,  1012,  4700,  2454,  1999,  2262,\n",
      "         1012, 20482,  1024,  1996,  4319,  1010, 15885,  1010,  2029,  2064,\n",
      "         2022, 20482,  2003,  8550,  6217,  1010,  3391,  2426,  2402,  3767,\n",
      "         1012,  3287,  2152,  2082,  2493,  2020,  1996,  2087,  3497,  2047,\n",
      "         5198,  1010,  5222, 17406,  2179,  1010,  1998,  2002,  2758,  3152,\n",
      "         2024,  2000,  7499,  2005,  6274,  7073,  2055,  1996,  4319,  1012,\n",
      "         1005,  7731,  3037,  2144,  1996,  2713,  1997,  1996,  8754,  2143,\n",
      "         1000,  4607,  1996, 11675,  1000,  1999,  2268,  1998,  1996,  2230,\n",
      "         4516,  1000,  1040, 20492,  1024,  1996,  4382, 13922,  1000,  1010,\n",
      "         2628,  2011,  1037,  3522,  3720,  1999,  1996,  6383,  3360,  2932,\n",
      "         3580,  3794,  2402,  2111,  2040,  2018,  2074, 20482,  1040, 20492,\n",
      "         2097,  2031,  2992,  7073,  1010,  1005,  5222, 17406,  2758,  1012,\n",
      "         1996,  3795,  4319,  5002,  2003,  2019, 10812,  2817,  2008,  3640,\n",
      "         1037,  4678,  2298,  2046,  1996,  6206,  5850,  2108,  2109,  2012,\n",
      "         2151,  2391,  1999,  2051,  1012,  2138,  1996,  2817,  3475,  1005,\n",
      "         1056,  6721,  1010,  2009,  2987,  1005,  1056,  2265,  1996, 20272,\n",
      "         1997,  3056,  5850,  2021,  2009,  2064,  2265,  1010, 20172,  1010,\n",
      "         2029,  3924,  4025,  2000,  2022,  2006,  1996,  4125,  1998,  2129,\n",
      "         2037,  5198,  2202,  2068,  1012,  2429,  2000,  1996,  2262,  2817,\n",
      "         1010,  1012,  4146,  2090,  2281,  1998,  2285,  2197,  2095,  1010,\n",
      "         1040, 20492,  4107,  2019,  1012,  1005,  6034,  2534, 14194, 28230,\n",
      "         3325,  1005,  1998,  5198, 21893,  1037,  1005,  2919,  4440,  1005,\n",
      "         2004,  1037,  1012,  4022,  3891,  1012,  1005,  1996,  1012,  3484,\n",
      "         1997,  5198,  6758,  1996,  3466,  1997,  1040, 20492,  2004,  6428,\n",
      "         2084, 17710, 15464,  3170,  1010,  1012,  3894, 23827,  1998,  1048,\n",
      "        16150,   102], device='cuda:0')\n",
      "t=tensor([  101, 27211, 16921,  7096,  2854, 22799, 11233,  1010,  2030,  1040,\n",
      "        20492,  1010,  2003,  1996,  3161, 21774,  1999,  1037, 17560,  6692,\n",
      "        15782,  1010,  2029,  2003,  1037,  3269,  1011,  2241,  8150,  2008,\n",
      "         2064,  2036,  2022, 20482,  1012,  1996,  4319,  4593,  3957,  5198,\n",
      "         1037,  2534, 14194, 28230,  1005,  4990,  1005,  2062,  6034,  2084,\n",
      "         2008,  1997,  1048, 16150,  1010, 17710, 15464,  3170,  2030,  3894,\n",
      "        23827,  1012,  4205,  5222, 17406,  1010,  3910,  1997,  1996,  3795,\n",
      "         5850,  5002,  1010,  4447,  1996,  4319,  2038,  1005,  1037,  3469,\n",
      "        10817,  1997,  2047,  5198,  1005,  4102,  2000,  2122,  2060,  3928,\n",
      "         5850,  1012,  2002,  2758,  2023,  6083,  1005,  2049,  6217,  2089,\n",
      "         3623,  1005,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n",
      "512\n",
      "123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/timoh/L90-Summarization/testing_lab3.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/timoh/L90-Summarization/testing_lab3.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/timoh/L90-Summarization/testing_lab3.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     train_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/timoh/L90-Summarization/testing_lab3.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     val_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/timoh/L90-Summarization/testing_lab3.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mhparams[\u001b[39m'\u001b[39;49m\u001b[39mlearning_rate\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/timoh/L90-Summarization/testing_lab3.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     grad_acc\u001b[39m=\u001b[39;49mhparams[\u001b[39m'\u001b[39;49m\u001b[39mgrad_acc\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/timoh/L90-Summarization/testing_lab3.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mhparams[\u001b[39m'\u001b[39;49m\u001b[39mnum_epochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/timoh/L90-Summarization/testing_lab3.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/L90-Summarization/models/abstractive_summarizer.py:141\u001b[0m, in \u001b[0;36mAbstractiveSummarizer.train\u001b[0;34m(self, train_dataloader, val_dataloader, learning_rate, grad_acc, num_epochs, keep_best_n_models)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMismatched batch sizes at batch \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m: input_ids has \u001b[39m\u001b[39m{\u001b[39;00minput_ids\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m elements, labels has \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m elements\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_ids, labels, src_key_padding_mask\u001b[39m=\u001b[39;49m(attention_mask\u001b[39m==\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    142\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(outputs, labels) \u001b[39m/\u001b[39m grad_acc\n\u001b[1;32m    143\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/L90-Summarization/models/abstractive_summarizer.py:102\u001b[0m, in \u001b[0;36mAbstractiveSummarizer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    100\u001b[0m src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(src)\n\u001b[1;32m    101\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(tgt)\n\u001b[0;32m--> 102\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\n\u001b[1;32m    103\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(output)\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/L90-Summarization/summarizer_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/L90-Summarization/summarizer_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/L90-Summarization/summarizer_venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:197\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    195\u001b[0m is_batched \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m tgt\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe batch number of src and tgt must be equal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m!=\u001b[39m tgt\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m    199\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe batch number of src and tgt must be equal\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    learning_rate=hparams['learning_rate'],\n",
    "    grad_acc=hparams['grad_acc'], \n",
    "    num_epochs=hparams['num_epochs'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
